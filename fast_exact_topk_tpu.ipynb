{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJWfO5nwfdzFn5xQISrkxd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oliverdutton/fast_exact_topk_tpu/blob/main/Fast_exact_topk_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorboard tensorboard-plugin-profile\n",
        "# Pip install will require a restart, then comment the code\n",
        "\n",
        "\n",
        "import functools\n",
        "import math\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import pallas as pl\n",
        "from jax.experimental.pallas import tpu as pltpu\n",
        "\n",
        "\n",
        "def block_topk(logits, k, topk_val=None, topk_index=None, start_k=0, num_lanes=128, mode='jax'):\n",
        "  ntokens = logits.shape[0]\n",
        "\n",
        "  if mode == 'jax':\n",
        "    topk_val = [jnp.full((ntokens, num_lanes), float('-inf'), dtype=logits.dtype) for i in range(k)]\n",
        "    # TODO?: Could use uint16 when vocab size < 4M if hardware supports\n",
        "    topk_index = [jnp.full((ntokens, num_lanes), -1, dtype=jnp.int32) for i in range(k)]\n",
        "  elif mode=='pallas':\n",
        "    if topk_val is None or topk_index is None:\n",
        "      raise ValueError(\"Pass through of topk_val and tok_index expected for pallas topk.\")\n",
        "\n",
        "  def while_body(i, while_carry):\n",
        "    topk_val, topk_index = while_carry\n",
        "\n",
        "    if mode == 'pallas':\n",
        "      vals_carry = logits[..., pl.dslice(num_lanes*i, num_lanes)]\n",
        "    elif mode == 'jax':\n",
        "      vals_carry = jax.lax.dynamic_slice_in_dim(logits, num_lanes*i, num_lanes, axis=1)\n",
        "    else:\n",
        "      raise ValueError(\"mode must be either `pallas` and a memory ref or `jax` and an array\")\n",
        "\n",
        "    index_carry = jnp.full((ntokens, num_lanes), i, jnp.int32)\n",
        "\n",
        "    for depth in range(k):\n",
        "      if depth < start_k:\n",
        "        # Nothing will be exchanged into the completed block topk, we just need\n",
        "        # to invalidate it from flowing downward. So we check if it's already\n",
        "        # found and invalidate if so.\n",
        "        vals_carry = jnp.where(index_carry == topk_index[depth], float('-inf'), vals_carry)\n",
        "      else:\n",
        "        # Sinking sort\n",
        "        mask = vals_carry > topk_val[depth]\n",
        "        # TODO: Consider packing bfloat16 val and uint16 index into single uint32 and packed sort as in https://github.com/triton-lang/triton/blob/main/python/triton_kernels/triton_kernels/topk_details/_topk_forward.py\n",
        "        topk_val[depth], vals_carry = (\n",
        "            jnp.where(m, vals_carry, topk_val[depth]) for m in (mask, ~mask))\n",
        "        topk_index[depth], index_carry = (\n",
        "            jnp.where(m, index_carry, topk_index[depth]) for m in (mask, ~mask))\n",
        "    return (topk_val, topk_index)\n",
        "\n",
        "  (topk_val, topk_index) = jax.lax.fori_loop(\n",
        "      0,\n",
        "      logits.shape[-1] // num_lanes,\n",
        "      while_body,\n",
        "      (topk_val, topk_index)\n",
        "  )\n",
        "  return topk_val, topk_index\n",
        "\n",
        "# Pallas kernel scaffolding\n",
        "def block_topk_kernel(logits_ref, topk_val_refs, topk_index_refs, max_depth_ref, flag_ref, num_lanes=128, k=64, depth_schedule=None):\n",
        "  ### Initialize refs\n",
        "  shape = topk_val_refs[0].shape\n",
        "  for i in range(len(topk_val_refs)):\n",
        "    topk_val_refs[i][...] = jnp.full(shape, float('-inf'), dtype=logits_ref.dtype)\n",
        "    topk_index_refs[i][...] = jnp.full(shape, -1, dtype=jnp.int32)\n",
        "\n",
        "  block_token = logits_ref.shape[0]\n",
        "  for i in range(block_token):\n",
        "    max_depth_ref[pl.program_id(0) * block_token + i] = -1\n",
        "\n",
        "  # flag for termination of while loop\n",
        "  flag_ref[0] = 0\n",
        "\n",
        "  ### Run increasing block topk, until sure overall topk present\n",
        "  if depth_schedule is None:\n",
        "    depth_schedule = (0, 5, 8, 12, k)\n",
        "\n",
        "  for completed_depth, depth in zip(depth_schedule, depth_schedule[1:]):\n",
        "    @pl.when(flag_ref[0] == 0)\n",
        "    def _():\n",
        "\n",
        "      topk_vals, topk_indexs = block_topk(\n",
        "          logits_ref,\n",
        "          # bf16, bf16 -> i1 mask not supported on v5e so we cast to f32\n",
        "          # TODO: check v6e, bf16 comparitor and make model specific\n",
        "          topk_val=jax.tree.map(lambda ref: ref[...].astype(jnp.float32), topk_val_refs),\n",
        "          topk_index=jax.tree.map(lambda ref: ref[...], topk_index_refs),\n",
        "          k=depth,\n",
        "          start_k=completed_depth,\n",
        "          mode='pallas',\n",
        "      )\n",
        "\n",
        "      for i in range(completed_depth, depth):\n",
        "        topk_val_refs[i][...] = topk_vals[i].astype(topk_val_refs[i].dtype)\n",
        "        topk_index_refs[i][...] = topk_indexs[i].astype(topk_index_refs[i].dtype)\n",
        "\n",
        "      # Stopping criterion check\n",
        "      # To find top-k values of a set, we can split into N subsets,\n",
        "      # and sort the largest, 2nd-largest, 3-rd largest, ..., m-th largest values for each subset\n",
        "      # When in the superset of top-(m-1) subsets there are more than k values\n",
        "      # larger (or equal than) the largest m'th largest value from the subsets\n",
        "      # then the top-(m-1) subsets must contain the top-k of the set.\n",
        "      # We run a schedule of m's until we have that full top-k found.\n",
        "      pivot_point = topk_vals[depth-1].max(-1, keepdims=True)\n",
        "      n_larger = sum(\n",
        "          [(v >= pivot_point) for v in topk_vals[:depth-1]]\n",
        "      ).astype(jnp.float32).sum(-1)\n",
        "      # flag SMEM used to check if all searches terminated\n",
        "      flag_ref[0] = 0\n",
        "      for i in range(block_token):\n",
        "        topk_all_present = n_larger[i] > k\n",
        "        flag_ref[0] += topk_all_present\n",
        "        # Store when the criteria was hit for each query\n",
        "        token_index = pl.program_id(0) * block_token + i\n",
        "        block_topk_depth = max_depth_ref[token_index]\n",
        "        max_depth_ref[token_index] = jnp.where(\n",
        "            topk_all_present & (block_topk_depth == -1),\n",
        "            depth - 1,\n",
        "            block_topk_depth)\n",
        "\n",
        "      # If not all terminated, reset the flag say we need to search deeper\n",
        "      @pl.when(flag_ref[0] != block_token)\n",
        "      def _():\n",
        "        flag_ref[0] = 0\n",
        "\n",
        "# Pallas function\n",
        "def block_topk_pallas(logits, k, num_lanes=128, block_token=None, depth_schedule=None):\n",
        "  num_tokens, vocab_size = logits.shape\n",
        "  if block_token is None:\n",
        "    block_token = min(32, num_tokens)\n",
        "  if num_tokens % block_token != 0:\n",
        "    raise ValueError('token block size must be a multiple of num tokens')\n",
        "\n",
        "  out_shape = (\n",
        "          [jax.ShapeDtypeStruct((num_tokens, num_lanes), logits.dtype) for i in range(k)],\n",
        "          [jax.ShapeDtypeStruct((num_tokens, num_lanes), jnp.int32) for i in range(k)], # uint16 fits vocab size of up to 2**16 * 128 = 8.4M. But not used to avoid unforseen issues.\n",
        "          jax.ShapeDtypeStruct((num_tokens,), jnp.int32), # block_topk required to be certain to contain topk vals\n",
        "          jax.ShapeDtypeStruct((1,), jnp.int32), # scratch for stopping boolean\n",
        "  )\n",
        "  out_specs = jax.tree.map(\n",
        "      lambda _: pl.BlockSpec((block_token, num_lanes), lambda i: (i, 0)),\n",
        "      out_shape[:2]\n",
        "  )\n",
        "  out_specs += (\n",
        "      pl.BlockSpec(memory_space=pltpu.SMEM),\n",
        "      pl.BlockSpec(memory_space=pltpu.SMEM),\n",
        "  )\n",
        "  return pl.pallas_call(\n",
        "      functools.partial(\n",
        "          block_topk_kernel,\n",
        "          k=k,\n",
        "          num_lanes=num_lanes,\n",
        "          depth_schedule=depth_schedule,\n",
        "      ),\n",
        "      in_specs=(\n",
        "          pl.BlockSpec((block_token, vocab_size), lambda i: (i, 0)),\n",
        "      ),\n",
        "      out_shape=out_shape,\n",
        "      grid=(num_tokens // block_token),\n",
        "      out_specs=out_specs,\n",
        "      debug=False,\n",
        "      compiler_params=pltpu.TPUCompilerParams(vmem_limit_bytes=2**26),\n",
        "  )(logits)\n",
        "\n",
        "def topk_on_filtered_subset(topk_val, topk_index, k):\n",
        "  num_lanes = topk_val[0].shape[-1]\n",
        "  topk_logits, local_indices = jax.lax.top_k(\n",
        "      jnp.concatenate(topk_val, axis=-1),\n",
        "      k=k\n",
        "  )\n",
        "\n",
        "  @jax.vmap\n",
        "  def unravel_indices(local_indices, topk_index):\n",
        "    depth, col = jnp.unravel_index(local_indices, (k, num_lanes))\n",
        "    row = jnp.stack(topk_index)[depth, col]\n",
        "    flat_index = row * num_lanes + col\n",
        "    return flat_index\n",
        "\n",
        "  topk_flat_indices = unravel_indices(local_indices, topk_index)\n",
        "  return topk_logits, topk_flat_indices\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnames=('k', 'base_cutoff', 'block_token', 'depth_schedule', 'num_lanes'))\n",
        "def topk_optimized(logits, k=64, base_cutoff=8, block_token=None, depth_schedule=None, num_lanes=128):\n",
        "  topk_val, topk_index, depths, _ = block_topk_pallas(logits, k=k, block_token=block_token, depth_schedule=depth_schedule, num_lanes=num_lanes)\n",
        "\n",
        "  # top-k the smallest number of values we can, by taking max depth required\n",
        "  # such that all queries in logits are guaranteed to have top-k\n",
        "  # We compile for a range of shapes, then use jax.lax.cond to run just one.\n",
        "  # in practice 8 nearly always sufficient\n",
        "  cutoff_schedule = [-1]+[\n",
        "      base_cutoff * (2**i) for i in range(int(math.log2(k // base_cutoff))+1)\n",
        "  ] + [k]\n",
        "\n",
        "  # Buffer for output to be written in to\n",
        "  topk_logits, topk_flat_indices = jax.tree.map(\n",
        "      jnp.zeros_like,\n",
        "      topk_on_filtered_subset(topk_val[:1], topk_index[:1], k=k)\n",
        "  )\n",
        "  for min_cutoff, cutoff in zip(cutoff_schedule, cutoff_schedule[1:]):\n",
        "    max_depth = depths.max()\n",
        "    topk_logits, topk_flat_indices = jax.lax.cond(\n",
        "        (max_depth > min_cutoff) & (max_depth <= cutoff),\n",
        "        lambda *args: topk_on_filtered_subset(topk_val=topk_val[:cutoff], topk_index=topk_index[:cutoff], k=k),\n",
        "        lambda *args: args,\n",
        "        topk_logits, topk_flat_indices\n",
        "    )\n",
        "  return topk_logits , topk_flat_indices"
      ],
      "metadata": {
        "id": "3ZqtXDgquFlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 64\n",
        "num_queries = 32\n",
        "vocab_size = 201088\n",
        "logits = jax.random.normal(jax.random.key(7), (num_queries, vocab_size), dtype=jnp.float32).astype(jnp.bfloat16)\n",
        "\n",
        "#\n",
        "all_values_match = (jax.lax.top_k(logits, 64)[0] == topk_optimized(logits, k=64)[0]).all()\n",
        "exact_index_match =  (jax.lax.top_k(logits, 64)[1] == topk_optimized(logits, k=64)[1]).mean()\n",
        "print(f'''All topk_logits match = {all_values_match}. Indices match at {exact_index_match:.0%} of the time,\n",
        "this is only O(40%) [not 100%] as bf16 has only 2**16=65k possible values so in 200k vocab size\n",
        "theres high degeneracy and sorting is different. Having checked, it looks correct. Writing full checks would be tricky.''')"
      ],
      "metadata": {
        "id": "zP2rOCeHVOB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lLBvtHqxACs"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "k = 64\n",
        "num_queries = 32\n",
        "vocab_size = 201088\n",
        "hidden_dim = 2880\n",
        "\n",
        "logit_key, key_act, key_weight = jax.random.split(jax.random.key(0), 3)\n",
        "x = jax.random.normal(key_act, (num_queries, hidden_dim), dtype=jnp.bfloat16)\n",
        "w = jax.random.normal(key_weight, (hidden_dim, vocab_size), dtype=jnp.bfloat16)\n",
        "logits = jax.random.normal(key_weight, (num_queries, vocab_size), dtype=jnp.float32).astype(jnp.bfloat16)\n",
        "\n",
        "topk_xla = jax.jit(jax.lax.top_k, static_argnames=('k',))\n",
        "approx_topk_xla = jax.jit(jax.lax.approx_max_k, static_argnames=('k',))\n",
        "\n",
        "@jax.jit\n",
        "@functools.partial(jax.vmap, in_axes=(0, None))\n",
        "def matmul_and_topk_xla(x, w, k=k):\n",
        "  logits = (x @ w)\n",
        "  return jax.lax.top_k(logits, k)\n",
        "\n",
        "def run():\n",
        "  # reference runtimes\n",
        "  o = jax.block_until_ready(x @ w)\n",
        "  jax.block_until_ready(matmul_and_topk_xla(x, w))\n",
        "  jax.block_until_ready(topk_xla(logits, k=k))\n",
        "\n",
        "  # optimized tpu run\n",
        "  jax.block_until_ready(topk_optimized(logits, k=k))\n",
        "\n",
        "  jax.block_until_ready(approx_topk_xla(logits, k=k))\n",
        "\n",
        "\n",
        "run()\n",
        "with jax.profiler.trace(\"/tmp/tensorboard\"):\n",
        "  run()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 32 tokens, top-64, v6e: topk_xla 1.32ms, topk_pallas_tpu 0.120ms (11x faster)\n",
        "# 2048 tokens, top-64, v6e: topk_xla 87.25ms, topk_pallas_tpu 7.23ms (12x faster)\n"
      ],
      "metadata": {
        "id": "yoQ8Gr_tTUC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorboard tensorboard-plugin-profile\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=/tmp/tensorboard"
      ],
      "metadata": {
        "id": "Dg71Zzdvuijy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Track how many iters you expect it to take\n",
        "# stopping_probs = (logits.reshape(logits.shape[0], -1, 128).sort(1).swapaxes(0,1)[::-1].max(-1) < jax.lax.top_k(logits, 64)[0].min(-1)).sum(-1)[:8] / logits.shape[0]"
      ],
      "metadata": {
        "id": "_sFaA8ehX39C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some maths calculating expectations of how far in subset top-m you need\n",
        "# to go to have filtered out all overall top-k values\n",
        "# Beware, this code is slow (as you might expect), so precomputed vals below\n",
        "\n",
        "\n",
        "# Each next largest value to be taken if the top-most not yet taken from one of the 128 lanes.\n",
        "# e.g. the second value taken is one of one of the 127 other top-1s or the 2nd largest of the taken lane.\n",
        "# and so on.\n",
        "\n",
        "def add_one(x, i):\n",
        "  if i == len(x):\n",
        "    # append to end\n",
        "    return x + (1,)\n",
        "  else:\n",
        "    # increment an intermediate val\n",
        "    return x[:i] + (x[i]+1,) + x[i+1:]\n",
        "\n",
        "def compute_termination_probabilities(k = 64, num_lanes = 128):\n",
        "  parent_states = {\n",
        "      (1,): 1.0, # only non zero values are in the hash\n",
        "  }\n",
        "  for generation in range(k-1):\n",
        "    child_states = {}\n",
        "    for state, parent_prob in parent_states.items():\n",
        "      for i in range(len(state)+1):\n",
        "        if i == 0:\n",
        "          child_prob = (1 - state[0] / num_lanes)\n",
        "        elif i == len(state):\n",
        "          child_prob = state[i-1] / num_lanes\n",
        "        else:\n",
        "          child_prob = (state[i-1] - state[i]) / num_lanes\n",
        "        if child_prob > 0:\n",
        "          child_prob *= parent_prob\n",
        "          child = add_one(state, i)\n",
        "          # print(state, i, child, child_prob)\n",
        "          if child in child_states:\n",
        "            child_states[child] += child_prob\n",
        "          else:\n",
        "            child_states[child] = child_prob\n",
        "    parent_states = child_states\n",
        "\n",
        "\n",
        "  probs = [0 for _ in range(k)]\n",
        "  for state, p in child_states.items():\n",
        "    probs[len(state)-1] += p\n",
        "  return probs\n",
        "\n",
        "# compute_termination_probabilities(k=64, num_lanes=128)\n",
        "# precomputed probs for k=64 and 128 lanes (random subsets being sorted)\n",
        "probs = jnp.array([\n",
        "4.1812404072740824e-09,\n",
        "0.13436890728003542,\n",
        "0.673656436392413,\n",
        "0.17274256944422797,\n",
        "0.01775253655062719,\n",
        "0.0013838922281360039,\n",
        "9.032679548553634e-05,\n",
        "5.067660246625492e-06,\n",
        "2.4828455416721553e-07,\n",
        "1.075248105864657e-08,\n",
        "4.156291895201961e-10,\n",
        "1.4454295961378757e-11,\n",
        "4.552534161066638e-13,\n",
        "1.3058450068301742e-14,\n",
        "3.427414716089682e-16,\n",
        "8.264927218917032e-18,\n",
        "1.8375011880871763e-19,\n",
        "3.77788958180651e-21,\n",
        "7.201944499092436e-23,\n",
        "1.2759350490518157e-24,\n",
        "2.1050297022227087e-26,\n",
        "3.239666327687077e-28,\n",
        "4.658198759426817e-30,\n",
        "6.265949774819566e-32,\n",
        "7.894109952528496e-34,\n",
        "9.323751912435309e-36,\n",
        "1.0332533469598683e-37,\n",
        "1.0750948773204598e-39,\n",
        "1.0508665648530184e-41,\n",
        "9.653629860854444e-44,\n",
        "8.33689142161678e-46,\n",
        "6.769621479167146e-48,\n",
        "5.168883019168442e-50,\n",
        "3.710870161978265e-52,\n",
        "2.5045242937986133e-54,\n",
        "1.5886090227506496e-56,\n",
        "9.466067809537837e-59,\n",
        "5.295976602932471e-61,\n",
        "2.7800402115131096e-63,\n",
        "1.368130025350939e-65,\n",
        "6.305957481932515e-68,\n",
        "2.719104276048889e-70,\n",
        "1.0954091571704027e-72,\n",
        "4.116605637182969e-75,\n",
        "1.4406318940272857e-77,\n",
        "4.6853827433273565e-80,\n",
        "1.412914883228222e-82,\n",
        "3.9402153895800056e-85,\n",
        "1.0130716090837228e-87,\n",
        "2.393082541142658e-90,\n",
        "5.1726347963559024e-93,\n",
        "1.0182351961330517e-95,\n",
        "1.81530565348338e-98,\n",
        "2.9116888580223354e-101,\n",
        "4.168487985715583e-104,\n",
        "5.275083221518592e-107,\n",
        "5.82962643626865e-110,\n",
        "5.53996538879725e-113,\n",
        "4.43611268287515e-116,\n",
        "2.9108350937500984e-119,\n",
        "1.502948286433509e-122,\n",
        "5.7262444238005166e-126,\n",
        "1.431382183177232e-129,\n",
        "1.761050914342067e-133])\n",
        "\n",
        "### Termination probs\n",
        "block_token = 32 # algorithm batched to sort 32 tokens of subsets\n",
        "probs.cumsum() ** block_token\n",
        "# by 128 top-8's you have 99.99998% chance of having the full\n",
        "# top-64 values for all 32 tokens"
      ],
      "metadata": {
        "id": "VAOC7EkVOc_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsKxCZp0WZ2K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
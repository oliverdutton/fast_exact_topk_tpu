{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "jnp = np\n",
        "# 65535 select left\n",
        "#select_upper = 65535 #jnp.packbits(jnp.arange(32) < 16).view(jnp.int32)\n",
        "# print(select_upper.dtype)\n",
        "def pack(val, index):\n",
        "  select_upper = 65535\n",
        "  # val and index both 16 bit dtypes bitcast to i32\n",
        "  l = ((val << 16) & ~select_upper) | (index & select_upper)\n",
        "  r = (val & ~select_upper) | ((index >> 16) & select_upper)\n",
        "  return (l, r)\n",
        "\n",
        "def unpack(l, r):\n",
        "  select_upper = 65535\n",
        "  vals, index = (\n",
        "   ((l >> 16) & select_upper) | (r & ~select_upper),\n",
        "   (l & select_upper) | ((r << 16) & ~select_upper)\n",
        "  )\n",
        "  return (vals, index)\n",
        "\n",
        "'''\n",
        "l, r = map(jnp.sort, pack(v.view(jnp.int32),i.view(jnp.int32)))\n",
        "print(v,i)\n",
        "vp, ip = unpack(l,r)\n",
        "print(vp.view(v.dtype)[::2], vp.view(v.dtype)[1::2], ip.view(i.dtype))\n",
        "'''\n",
        "### start\n",
        "\n",
        "import functools\n",
        "import math\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import pallas as pl\n",
        "from jax.experimental.pallas import tpu as pltpu\n",
        "\n",
        "select_upper = 65535 #jnp.array(65535, dtype=jnp.int32) # jnp.packbits(jnp.arange(32) < 16).view(jnp.int32)\n",
        "\n",
        "def _pack(val, index):\n",
        "  # val and index both 16 bit dtypes bitcast to i32\n",
        "  l = ((val << 16) & ~select_upper) | (index & select_upper)\n",
        "  r = (val & ~select_upper) | ((index >> 16) & select_upper)\n",
        "  return (l, r)\n",
        "\n",
        "def _unpack(l, r):\n",
        "  vals, index = (\n",
        "   ((l >> 16) & select_upper) | (r & ~select_upper),\n",
        "   (l & select_upper) | ((r << 16) & ~select_upper)\n",
        "  )\n",
        "  return (vals, index)\n",
        "\n",
        "def blockwise_topk_packed(\n",
        " logits,\n",
        " k,\n",
        " block_topk_val=None,\n",
        " block_topk_index=None,\n",
        " start_k=0,\n",
        " num_blocks=128,\n",
        " mode='jax',\n",
        "):\n",
        " '''Compute blockwise top-k.'''\n",
        " ntokens = logits.shape[0]\n",
        " assert (logits.dtype==jnp.int32) and (pl.cdiv(logits.shape[1], num_blocks) < jnp.iinfo(jnp.uint16).max)\n",
        "\n",
        " # pack\n",
        " # vals are in bf16, indexs in uint16 but both bitcast to int32\n",
        " block_topk_packed = []\n",
        " for i in range(k):\n",
        "   block_topk_packed.append(\n",
        "     list(_pack(block_topk_val[i], block_topk_index[i]))\n",
        "   )\n",
        "\n",
        " def while_body(block_i, while_carry):\n",
        "   block_topk_packeds = while_carry\n",
        "\n",
        "   if mode == 'pallas':\n",
        "     val_carry = logits[..., pl.dslice(num_blocks * block_i, num_blocks)]\n",
        "     print(val_carry.shape, logits.shape, val_carry.dtype, logits.dtype)\n",
        "   else:\n",
        "     raise ValueError(\n",
        "       'mode must be either `pallas` and a memory ref or `jax` and an array'\n",
        "     )\n",
        "   # block_i in int32 is the same as uint16 value padded to 32 bits\n",
        "   index_carry = block_i #jnp.array(block_i, dtype=jnp.int32)\n",
        "\n",
        "   bubbles = (\n",
        "     ((val_carry << 16) & ~select_upper) | index_carry,\n",
        "     (val_carry & ~select_upper) | index_carry\n",
        "   )\n",
        "\n",
        "   for i, bubble in enumerate(bubbles):\n",
        "     for j in range(k):\n",
        "       if j < start_k:\n",
        "         # Nothing will be exchanged into the completed block topk, we just need\n",
        "         # to invalidate it from flowing downward. So we check if it's already\n",
        "         # found and invalidate if so.\n",
        "         bubble = jnp.where(\n",
        "           bubble == block_topk_packed[j][i], jnp.iinfo(jnp.int32).min, bubble\n",
        "         )\n",
        "       else:\n",
        "         # Sinking bubble sort\n",
        "         mask = bubble > block_topk_packed[j][i]\n",
        "         block_topk_packed[j][i], bubble = (\n",
        "           jnp.where(v, bubble , block_topk_packed[j][i]) for v in (mask, ~mask)\n",
        "         )\n",
        "   return block_topk_packed\n",
        " block_topk_packed = jax.lax.fori_loop(0, logits.shape[-1] // num_blocks, while_body, block_topk_packed)\n",
        " # unpack\n",
        " for i in range(start_k, k):\n",
        "    block_topk_val[i], block_topk_index[i] = _unpack(*block_topk_packed[i])\n",
        " return block_topk_val, block_topk_index\n",
        "\n",
        "\n",
        "# Pallas kernel scaffolding\n",
        "def topk_blockwise_superset_kernel_packed(\n",
        " logits_ref,\n",
        " block_topm_val_refs,\n",
        " block_topm_index_refs,\n",
        " max_m_ref,\n",
        " flag_ref,\n",
        " num_blocks: int = 128,\n",
        " k: int = 64,\n",
        " m_schedule: tuple[int] | None = None,\n",
        "):\n",
        " '''Compute blockwise top-m's until they contain global top-k.'''\n",
        " ### Initialize refs\n",
        " assert logits_ref.dtype == jnp.bfloat16\n",
        " shape = block_topm_val_refs[0].shape\n",
        " for i in range(len(block_topm_val_refs)):\n",
        "   block_topm_val_refs[i][...] = jnp.full(shape, float('-inf'), dtype=logits_ref.dtype)\n",
        "   block_topm_index_refs[i][...] = jnp.full(shape, jnp.iinfo(jnp.uint16).max, dtype=jnp.uint16)\n",
        "\n",
        " block_token = logits_ref.shape[0]\n",
        " for i in range(block_token):\n",
        "   # Worst case m = k\n",
        "   max_m_ref[pl.program_id(0) * block_token + i] = k\n",
        "\n",
        " # flag for termination of while loop\n",
        " flag_ref[0] = 0\n",
        "\n",
        " ### Run increasing block topk, until sure overall topk present\n",
        " if m_schedule is None:\n",
        "   m_schedule = (5, 8, 12)\n",
        " # Ensure worst case of all k in one block is covered\n",
        " m_schedule = (0,) + m_schedule + (k,)\n",
        "\n",
        " for completed_m, m in zip(m_schedule, m_schedule[1:]):\n",
        "\n",
        "   @pl.when(flag_ref[0] == 0)\n",
        "   def _():\n",
        "     topk_vals, topk_indexs = blockwise_topk_packed(\n",
        "       logits_ref.bitcast(jnp.int32),\n",
        "       block_topk_val=jax.tree.map(\n",
        "         lambda ref: ref.bitcast(jnp.int32)[...], block_topm_val_refs\n",
        "       ),\n",
        "       block_topk_index=jax.tree.map(lambda ref: ref.bitcast(jnp.int32)[...], block_topm_index_refs),\n",
        "       k=m,\n",
        "       start_k=completed_m,\n",
        "       mode='pallas',\n",
        "     )\n",
        "\n",
        "     for i in range(completed_m, m):\n",
        "       block_topm_val_refs[i].bitcast(jnp.int32)[...] = topk_vals[i]\n",
        "       block_topm_index_refs[i].bitcast(jnp.int32)[...] = topk_indexs[i]\n",
        "\n",
        "     # Stopping criterion check\n",
        "     # To find top-k values of a set, we can split into N subsets,\n",
        "     # and sort the largest, 2nd-largest, 3-rd largest, ..., m-th largest values for each subset\n",
        "     # When in the superset of top-(m-1) subsets there are more than k values\n",
        "     # larger (or equal than) the largest m'th largest value from the subsets\n",
        "     # then the top-(m-1) subsets must contain the top-k of the set.\n",
        "     # We run a schedule of m's until we have that full top-k found.\n",
        "     pivot_point = block_topm_val_refs[m - 1][...].astype(jnp.float32).max(-1, keepdims=True)\n",
        "     n_larger = (\n",
        "       sum([(v[...].astype(jnp.float32) >= pivot_point) for v in block_topm_val_refs[: m - 1]])\n",
        "       .astype(jnp.float32)\n",
        "       .sum(-1)\n",
        "     )\n",
        "     # flag SMEM used to check if all searches terminated\n",
        "     flag_ref[0] = 0\n",
        "     for i in range(block_token):\n",
        "       blockwise_topm_contains_topk = n_larger[i] >= k\n",
        "       flag_ref[0] += blockwise_topm_contains_topk\n",
        "       # Store when the criteria was hit for each query\n",
        "       token_index = pl.program_id(0) * block_token + i\n",
        "       max_m = max_m_ref[token_index]\n",
        "       max_m_ref[token_index] = jnp.where(\n",
        "         blockwise_topm_contains_topk & (max_m == k), m - 1, max_m\n",
        "       )\n",
        "\n",
        "     # If not all terminated, reset the flag say we need to search deeper\n",
        "     @pl.when(flag_ref[0] != block_token)\n",
        "     def _():\n",
        "       flag_ref[0] = 0\n",
        "\n",
        "\n",
        "# Pallas function\n",
        "def topk_blockwise_superset_pallas(\n",
        " logits, k, num_blocks=128, block_token=None, m_schedule=None\n",
        "):\n",
        " num_tokens, vocab_size = logits.shape\n",
        " if block_token is None:\n",
        "   block_token = min(32, num_tokens)\n",
        " if num_tokens % block_token != 0:\n",
        "   raise ValueError('token block size must be a multiple of num tokens')\n",
        "\n",
        " out_shape = (\n",
        "   [jax.ShapeDtypeStruct((num_tokens, num_blocks), logits.dtype) for i in range(k)],\n",
        "   # uint16 fits vocab size of up to 2**16 * 128 = 8.4M. But not used to avoid unforseen issues.\n",
        "   [jax.ShapeDtypeStruct((num_tokens, num_blocks), jnp.uint16) for i in range(k)],\n",
        "   jax.ShapeDtypeStruct((num_tokens,), jnp.int32),\n",
        "   jax.ShapeDtypeStruct((1,), jnp.int32),  # scratch for termination flag\n",
        " )\n",
        " out_specs = jax.tree.map(\n",
        "   lambda _: pl.BlockSpec((block_token, num_blocks), lambda i: (i, 0)), out_shape[:2]\n",
        " )\n",
        " out_specs += (\n",
        "   pl.BlockSpec(memory_space=pltpu.SMEM),\n",
        "   pl.BlockSpec(memory_space=pltpu.SMEM),\n",
        " )\n",
        " return pl.pallas_call(\n",
        "   functools.partial(\n",
        "     topk_blockwise_superset_kernel_packed,\n",
        "     k=k,\n",
        "     num_blocks=num_blocks,\n",
        "     m_schedule=m_schedule,\n",
        "   ),\n",
        "   in_specs=(pl.BlockSpec((block_token, vocab_size), lambda i: (i, 0)),),\n",
        "   out_shape=out_shape,\n",
        "   grid=(num_tokens // block_token),\n",
        "   out_specs=out_specs,\n",
        "   compiler_params=pltpu.TPUCompilerParams(vmem_limit_bytes=2**26),\n",
        " )(logits)\n",
        "\n",
        "\n",
        "def topk_on_filtered_subset(block_topm_val, block_topm_index, k):\n",
        " num_blocks = block_topm_val[0].shape[-1]\n",
        " topk_logits, local_indices = jax.lax.top_k(\n",
        "   jnp.concatenate(block_topm_val, axis=-1), k=k\n",
        " )\n",
        "\n",
        " @jax.vmap\n",
        " def unravel_indices(local_indices, block_topm_index):\n",
        "   # TODO: consider rewriting as lane permutations\n",
        "   m, col = jnp.unravel_index(local_indices, (k, num_blocks))\n",
        "   row = jnp.stack(block_topm_index)[m, col]\n",
        "   flat_index = row.astype(jnp.int32) * num_blocks + col.astype(jnp.int32)\n",
        "   return flat_index\n",
        "\n",
        " topk_flat_indices = unravel_indices(local_indices, block_topm_index)\n",
        " return topk_logits, topk_flat_indices\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        " jax.jit,\n",
        " static_argnames=(\n",
        "   'k',\n",
        "   'num_blocks',\n",
        "   'm_stage1_schedule',\n",
        "   'm_stage2_schedule',\n",
        "   'block_token',\n",
        " ),\n",
        ")\n",
        "def topk_optimized(\n",
        " logits,\n",
        " k: int = 64,\n",
        " num_blocks: int = 128,\n",
        " m_stage1_schedule: tuple[int] | None = None,\n",
        " m_stage2_schedule: tuple[int] | None = None,\n",
        " block_token: int | None = None,\n",
        "):\n",
        " '''Fast implementation of jax.lax.top_k on TPUs.'''\n",
        " if logits.ndim != 2:\n",
        "   raise ValueError('Expected 2D input')\n",
        " block_topm_val, block_topm_index, termination_m, _ = topk_blockwise_superset_pallas(\n",
        "   logits,\n",
        "   k=k,\n",
        "   block_token=block_token,\n",
        "   m_schedule=m_stage1_schedule,\n",
        "   num_blocks=num_blocks,\n",
        " )\n",
        " jax.debug.print('{} {} {}', termination_m, block_topm_val[::64], block_topm_index[::64])\n",
        "\n",
        " # top-k the smallest number of values we can, by taking max m required\n",
        " # such that all queries to have full top-k\n",
        " # We compile for a range of shapes, then use jax.lax.cond to run just one.\n",
        " # in practice 8 nearly always sufficient\n",
        " if m_stage2_schedule is None:\n",
        "   m_init = 8\n",
        "   m_stage2_schedule = [\n",
        "     m_init * (2**i) for i in range(int(math.log2(k // m_init)) + 1)\n",
        "   ]\n",
        " # Guarantee all cases covered\n",
        " m_stage2_schedule = (-1,) + tuple(m_stage2_schedule) + (k,)\n",
        "\n",
        " # Buffer for output to be written in to\n",
        " topk_logits, topk_flat_indices = jax.tree.map(\n",
        "   jnp.zeros_like,\n",
        "   topk_on_filtered_subset(block_topm_val[:1], block_topm_index[:1], k=k),\n",
        " )\n",
        " max_m = termination_m.max()\n",
        " for lower_m, upper_m in zip(m_stage2_schedule, m_stage2_schedule[1:]):\n",
        "   topk_logits, topk_flat_indices = jax.lax.cond(\n",
        "     (max_m > lower_m) & (max_m <= upper_m),\n",
        "     lambda *args: topk_on_filtered_subset(\n",
        "       block_topm_val=block_topm_val[:upper_m],\n",
        "       block_topm_index=block_topm_index[:upper_m],\n",
        "       k=k,\n",
        "     ),\n",
        "     lambda *args: args,\n",
        "     topk_logits,\n",
        "     topk_flat_indices,\n",
        "   )\n",
        " # the packed sort is only valid if all topk are >=0 because we do a i32\n",
        " # sort with the bf16 bits packed which sorts negative values in reverse order\n",
        " # if this wasnt true we fallback to a classic slow topk\n",
        " # TODO: revert to f32 and i32 block_topk rather than packed version\n",
        " '''\n",
        " topk_logits, topk_flat_indices = jax.lax.cond(\n",
        "   (topk_logits[:,-1] >= 0).all(),\n",
        "   lambda *args: args,\n",
        "   lambda *args: tuple(jax.lax.top_k(logits, k)),\n",
        "   (topk_logits, topk_flat_indices)\n",
        " )\n",
        " '''\n",
        " return topk_logits, topk_flat_indices\n",
        "\n"
      ],
      "metadata": {
        "id": "o0hYh1XJtlNN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topk_optimized(logits, 64, m_stage1_schedule=())"
      ],
      "metadata": {
        "id": "pGIuHqypnWKT",
        "outputId": "6699482f-073d-4b99-84ac-a4fc8ba45551",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 128) (16, 201088) int32 int32\n",
            "[63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63\n",
            " 63 63 63 63 63 63 63 63] [Array([[1.02344, 0.546875, -inf, ..., 0.259766, -inf, 0.835938],\n",
            "       [-inf, 0.558594, -inf, ..., -inf, -inf, 0.691406],\n",
            "       [0.613281, -inf, -inf, ..., 0.036377, -inf, -inf],\n",
            "       ...,\n",
            "       [0.578125, 1.69531, 0.425781, ..., 0.40625, -inf, 1.14844],\n",
            "       [-inf, -inf, -inf, ..., 0.609375, -inf, -inf],\n",
            "       [-inf, 0.237305, 0.628906, ..., 1.30469, 0.275391, -inf]],      dtype=bfloat16)] [Array([[ 1570,  1570, 65535, ...,  1570, 65535,  1570],\n",
            "       [65535,  1570, 65535, ..., 65535, 65535,  1570],\n",
            "       [ 1570, 65535, 65535, ...,  1570, 65535, 65535],\n",
            "       ...,\n",
            "       [ 1570,  1570,  1570, ...,  1570, 65535,  1570],\n",
            "       [65535, 65535, 65535, ...,  1570, 65535, 65535],\n",
            "       [65535,  1570,  1570, ...,  1570,  1570, 65535]], dtype=uint16)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([[2.17188, 2.125, 1.91406, ..., 0.136719, 0.132812, 0.123535],\n",
              "        [2.98438, 2.5625, 2.45312, ..., 0.253906, 0.249023, 0.237305],\n",
              "        [2.89062, 2.14062, 1.71875, ..., 0.0361328, 0.0115967, 0.00494385],\n",
              "        ...,\n",
              "        [3.20312, 2.85938, 2.70312, ..., 0.223633, 0.199219, 0.176758],\n",
              "        [2.75, 1.85938, 1.78125, ..., 0.026123, 0.0115356, -inf],\n",
              "        [1.95312, 1.9375, 1.89062, ..., 0.0201416, 0.00921631, 0.00169373]],      dtype=bfloat16),\n",
              " Array([[ 201076,  200975,  201004, ...,  201075,  201003,  200991],\n",
              "        [ 201084,  200967,  201021, ...,  201073,  200979,  201066],\n",
              "        [ 201036,  201009,  200965, ...,  200970,  201070,  201026],\n",
              "        ...,\n",
              "        [ 201010,  201038,  200980, ...,  201059,  200998,  201029],\n",
              "        [ 201006,  201054,  201004, ...,  201079,  201019, 8388480],\n",
              "        [ 201068,  201032,  201057, ...,  201080,  201075,  200966]],      dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "k = 64\n",
        "num_queries = 128\n",
        "vocab_size = 201088\n",
        "\n",
        "# To get large value range, randint across uint16, then bitcast to bfloat16, then remove non-normal values\n",
        "dtype = jnp.uint16\n",
        "logits = jax.lax.bitcast_convert_type(jax.random.randint(jax.random.key(7), (num_queries, vocab_size), dtype=dtype, minval=jnp.iinfo(dtype).min, maxval=jnp.iinfo(dtype).max), jnp.bfloat16)\n",
        "logits = jnp.where(jnp.isnan(logits) | (logits==jnp.inf), 0, logits) # remove the nans and +infs\n",
        "\n",
        "logits = jax.random.normal(jax.random.key(0), logits.shape).astype(jnp.bfloat16)\n",
        "\n",
        "logits_2 = jnp.ones_like(logits)\n",
        "# Adversarial logits, in practice this is astronomically unlikely\n",
        "logits_worst_case = jnp.zeros((num_queries, vocab_size)).at[...,::128].set(1.)\n",
        "\n",
        "all_values_match = (jax.lax.top_k(logits, k)[0] == topk_optimized(logits, k=k)[0]).all()\n",
        "exact_index_match =  (jax.lax.top_k(logits, k)[1] == topk_optimized(logits, k=k)[1]).mean()\n",
        "print(f'''All topk_logits match = {all_values_match}. Indices match at {exact_index_match:.0%} of the time,\n",
        "this is only O(40%) [not 100%] as bf16 has only 2**16=65k possible values so in 200k vocab size\n",
        "theres high degeneracy and sorting is different. Having checked, it looks correct. Writing full checks would be tricky.''')\n"
      ],
      "metadata": {
        "id": "zP2rOCeHVOB7",
        "outputId": "e30fdc13-05d1-4b18-ad54-66a9bc586b63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4] [Array([[1.17969, -inf, -inf, ..., -inf, -inf, -inf],\n",
            "       [1.67969, -inf, -inf, ..., 1.16406, -inf, -inf],\n",
            "       [0.0600586, 0.574219, -inf, ..., 0.414062, 0.0810547, 1.15625],\n",
            "       ...,\n",
            "       [-inf, 0.178711, 1.07031, ..., 0.890625, -inf, -inf],\n",
            "       [0.746094, -inf, -inf, ..., -inf, 0.390625, 0.496094],\n",
            "       [-inf, 0.349609, -inf, ..., 0.550781, -inf, 0.170898]],      dtype=bfloat16)] [Array([[ 1570, 65535, 65535, ..., 65535, 65535, 65535],\n",
            "       [ 1570, 65535, 65535, ...,  1570, 65535, 65535],\n",
            "       [ 1570,  1570, 65535, ...,  1570,  1570,  1570],\n",
            "       ...,\n",
            "       [65535,  1570,  1570, ...,  1570, 65535, 65535],\n",
            "       [ 1570, 65535, 65535, ..., 65535,  1570,  1570],\n",
            "       [65535,  1570, 65535, ...,  1570, 65535,  1570]], dtype=uint16)]\n",
            "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4] [Array([[1.17969, -inf, -inf, ..., -inf, -inf, -inf],\n",
            "       [1.67969, -inf, -inf, ..., 1.16406, -inf, -inf],\n",
            "       [0.0600586, 0.574219, -inf, ..., 0.414062, 0.0810547, 1.15625],\n",
            "       ...,\n",
            "       [-inf, 0.178711, 1.07031, ..., 0.890625, -inf, -inf],\n",
            "       [0.746094, -inf, -inf, ..., -inf, 0.390625, 0.496094],\n",
            "       [-inf, 0.349609, -inf, ..., 0.550781, -inf, 0.170898]],      dtype=bfloat16)] [Array([[ 1570, 65535, 65535, ..., 65535, 65535, 65535],\n",
            "       [ 1570, 65535, 65535, ...,  1570, 65535, 65535],\n",
            "       [ 1570,  1570, 65535, ...,  1570,  1570,  1570],\n",
            "       ...,\n",
            "       [65535,  1570,  1570, ...,  1570, 65535, 65535],\n",
            "       [ 1570, 65535, 65535, ..., 65535,  1570,  1570],\n",
            "       [65535,  1570, 65535, ...,  1570, 65535,  1570]], dtype=uint16)]\n",
            "All topk_logits match = False. Indices match at 0% of the time,\n",
            "this is only O(40%) [not 100%] as bf16 has only 2**16=65k possible values so in 200k vocab size\n",
            "theres high degeneracy and sorting is different. Having checked, it looks correct. Writing full checks would be tricky.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4lLBvtHqxACs",
        "outputId": "d561d48f-55ce-4e66-b2a3-11c8cdc51c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 128) (16, 201088) int32 int32\n",
            "(16, 128) (16, 201088) int32 int32\n",
            "(16, 128) (16, 201088) int32 int32\n",
            "(16, 128) (16, 201088) int32 int32\n",
            "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4] [Array([[1.02344, 0.546875, -inf, ..., 0.259766, -inf, 0.835938],\n",
            "       [-inf, 0.558594, -inf, ..., -inf, -inf, 0.691406],\n",
            "       [0.613281, -inf, -inf, ..., 0.036377, -inf, -inf],\n",
            "       ...,\n",
            "       [0.578125, 1.69531, 0.425781, ..., 0.40625, -inf, 1.14844],\n",
            "       [-inf, -inf, -inf, ..., 0.609375, -inf, -inf],\n",
            "       [-inf, 0.237305, 0.628906, ..., 1.30469, 0.275391, -inf]],      dtype=bfloat16)] [Array([[ 1570,  1570, 65535, ...,  1570, 65535,  1570],\n",
            "       [65535,  1570, 65535, ..., 65535, 65535,  1570],\n",
            "       [ 1570, 65535, 65535, ...,  1570, 65535, 65535],\n",
            "       ...,\n",
            "       [ 1570,  1570,  1570, ...,  1570, 65535,  1570],\n",
            "       [65535, 65535, 65535, ...,  1570, 65535, 65535],\n",
            "       [65535,  1570,  1570, ...,  1570,  1570, 65535]], dtype=uint16)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1680114971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/tensorboard\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1680114971.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;31m# Optimized tpu on adversarial logits, to check astronomically unlikely worst case runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_until_ready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopk_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_worst_case\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;31m# Not exact. Runtime varies with recall, here run with default 0.95\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-886484481.py\u001b[0m in \u001b[0;36mtopk_optimized\u001b[0;34m(logits, k, num_blocks, m_stage1_schedule, m_stage2_schedule, block_token)\u001b[0m\n\u001b[1;32m    273\u001b[0m  \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m    \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected 2D input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m  block_topm_val, block_topm_index, termination_m, _ = topk_blockwise_superset_pallas(\n\u001b[0m\u001b[1;32m    276\u001b[0m    \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m    \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-886484481.py\u001b[0m in \u001b[0;36mtopk_blockwise_superset_pallas\u001b[0;34m(logits, k, num_blocks, block_token, m_schedule)\u001b[0m\n\u001b[1;32m    219\u001b[0m    \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlockSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpltpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSMEM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m  )\n\u001b[0;32m--> 221\u001b[0;31m  return pl.pallas_call(\n\u001b[0m\u001b[1;32m    222\u001b[0m    functools.partial(\n\u001b[1;32m    223\u001b[0m      \u001b[0mtopk_blockwise_superset_kernel_packed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/pallas/pallas_call.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m       \u001b[0mkernel_dbg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_dbg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_func_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msanitize_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m     jaxpr, consts = _trace_kernel_to_jaxpr(\n\u001b[0m\u001b[1;32m   1657\u001b[0m         \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_dbg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_kernel_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         kernel_in_tree, kernel_arg_transforms)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/pallas/pallas_call.py\u001b[0m in \u001b[0;36m_trace_kernel_to_jaxpr\u001b[0;34m(fun, debug_info, grid_mapping, kernel_avals, kernel_in_tree, kernel_in_transforms, indexer)\u001b[0m\n\u001b[1;32m   1189\u001b[0m   )\n\u001b[1;32m   1190\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mgrid_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m     jaxpr, _, consts, () = pe.trace_to_jaxpr_dynamic(wrapped_kernel_fun,\n\u001b[0m\u001b[1;32m   1192\u001b[0m                                                      kernel_avals)\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/pallas/primitives.py\u001b[0m in \u001b[0;36mwrap_with_transforms\u001b[0;34m(f, transforms, *args)\u001b[0m\n\u001b[1;32m    872\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m   )\n\u001b[0;32m--> 874\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-886484481.py\u001b[0m in \u001b[0;36mtopk_blockwise_superset_kernel_packed\u001b[0;34m(logits_ref, block_topm_val_refs, block_topm_index_refs, max_m_ref, flag_ref, num_blocks, k, m_schedule)\u001b[0m\n\u001b[1;32m    125\u001b[0m  \u001b[0;34m'''Compute blockwise top-m's until they contain global top-k.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m  \u001b[0;31m### Initialize refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m  \u001b[0;32massert\u001b[0m \u001b[0mlogits_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m  \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_topm_val_refs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_topm_val_refs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "k = 64\n",
        "num_queries = 32\n",
        "vocab_size = 201088\n",
        "hidden_dim = 2880\n",
        "\n",
        "logit_key, key_act, key_weight = jax.random.split(jax.random.key(0), 3)\n",
        "x = jax.random.normal(key_act, (num_queries, hidden_dim), dtype=jnp.bfloat16)\n",
        "w = jax.random.normal(key_weight, (hidden_dim, vocab_size), dtype=jnp.bfloat16)\n",
        "logits = jax.random.normal(key_weight, (num_queries, vocab_size), dtype=jnp.float32).astype(jnp.bfloat16)\n",
        "\n",
        "topk_xla = jax.jit(jax.lax.top_k, static_argnames=('k',))\n",
        "approx_topk_xla = jax.jit(jax.lax.approx_max_k, static_argnames=('k',))\n",
        "\n",
        "@jax.jit\n",
        "@functools.partial(jax.vmap, in_axes=(0, None))\n",
        "def matmul_and_topk_xla(x, w, k=k):\n",
        "  logits = (x @ w)\n",
        "  return jax.lax.top_k(logits, k)\n",
        "\n",
        "def run():\n",
        "  # reference runtimes\n",
        "  o = jax.block_until_ready(x @ w)\n",
        "  jax.block_until_ready(matmul_and_topk_xla(x, w))\n",
        "  jax.block_until_ready(topk_xla(logits, k=k))\n",
        "\n",
        "  # Optimized tpu run on random logits\n",
        "  jax.block_until_ready(topk_optimized(logits, k=k))\n",
        "\n",
        "  # Optimized tpu on adversarial logits, to check astronomically unlikely worst case runtime\n",
        "  jax.block_until_ready(topk_optimized(logits_worst_case, k=k))\n",
        "\n",
        "  # Not exact. Runtime varies with recall, here run with default 0.95\n",
        "  jax.block_until_ready(approx_topk_xla(logits, k=k))\n",
        "\n",
        "\n",
        "\n",
        "run()\n",
        "with jax.profiler.trace(\"/tmp/tensorboard\"):\n",
        "  run()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 32 tokens, top-64, v5e: topk_xla 1.32ms, topk_pallas_tpu 0.120ms (11x faster)\n",
        "# 2048 tokens, top-64, v5e: topk_xla 87.25ms, topk_pallas_tpu 7.23ms (12x faster)\n"
      ],
      "metadata": {
        "id": "yoQ8Gr_tTUC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.array(6, jnp.int32) << 16"
      ],
      "metadata": {
        "id": "lsKxCZp0WZ2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf04a3e-3c94-42db-95e7-e10d31ed2228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(393216, dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}
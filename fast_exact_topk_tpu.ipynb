{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import functools\n",
        "import math\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import pallas as pl\n",
        "from jax.experimental.pallas import tpu as pltpu\n",
        "\n",
        "def unrolled_fori_loop(length, body_fn, init, unroll):\n",
        " def unrolled_body(i, c):\n",
        "   i *= unroll\n",
        "   for j in range(unroll):\n",
        "     c = body_fn(i + j, c)\n",
        "   return c\n",
        " carry = jax.lax.fori_loop(0, length // unroll, unrolled_body, init)\n",
        " for j in range(length % unroll):\n",
        "   carry = body_fn((length // unroll) * unroll + j, carry)\n",
        " return carry\n",
        "\n",
        "def blockwise_topk(\n",
        "  logits,\n",
        "  k,\n",
        "  block_topk_val=None,\n",
        "  block_topk_index=None,\n",
        "  start_k=0,\n",
        "  num_blocks=128,\n",
        "  mode='jax',\n",
        "):\n",
        "  '''Compute blockwise top-k.'''\n",
        "  ntokens = logits.shape[0]\n",
        "\n",
        "  if start_k != 0 and (block_topk_val is None or block_topk_index is None):\n",
        "    raise ValueError(\n",
        "      'start_k must be 0, unless precomputed top-(start_k) in a buffer is provided in block_topk_val and block_topk_index'\n",
        "    )\n",
        "  if mode == 'jax':\n",
        "    block_topk_val = [\n",
        "      jnp.full((ntokens, num_blocks), float('-inf'), dtype=logits.dtype)\n",
        "      for i in range(k)\n",
        "    ]\n",
        "    # TODO?: Could use uint16 when vocab size < 4M if hardware supports\n",
        "    block_topk_index = [\n",
        "      jnp.full((ntokens, num_blocks), -1, dtype=jnp.int32) for i in range(k)\n",
        "    ]\n",
        "  elif mode == 'pallas':\n",
        "    if block_topk_val is None or block_topk_index is None:\n",
        "      raise ValueError(\n",
        "        'Pass through of block_topk_val and tok_index expected for pallas topk.'\n",
        "      )\n",
        "\n",
        "  def while_body(i, while_carry):\n",
        "    block_topk_val, block_topk_index = while_carry\n",
        "\n",
        "    if mode == 'pallas':\n",
        "      vals_carry = logits[..., pl.dslice(num_blocks * i, num_blocks)]\n",
        "    elif mode == 'jax':\n",
        "      vals_carry = jax.lax.dynamic_slice_in_dim(\n",
        "        logits, num_blocks * i, num_blocks, axis=1\n",
        "      )\n",
        "    else:\n",
        "      raise ValueError(\n",
        "        'mode must be either `pallas` and a memory ref or `jax` and an array'\n",
        "      )\n",
        "\n",
        "    index_carry = jnp.full((ntokens, num_blocks), i, jnp.int32)\n",
        "\n",
        "    for i in range(k):\n",
        "      if i < start_k:\n",
        "        # Nothing will be exchanged into the completed block topk, we just need\n",
        "        # to invalidate it from flowing downward. So we check if it's already\n",
        "        # found and invalidate if so.\n",
        "        vals_carry = jnp.where(\n",
        "          index_carry == block_topk_index[i], float('-inf'), vals_carry\n",
        "        )\n",
        "      else:\n",
        "        # Sinking sort\n",
        "        mask = vals_carry > block_topk_val[i]\n",
        "        # TODO: Consider packing bfloat16 val and uint16 index into single uint32\n",
        "        # and packed sort as in\n",
        "        # https://github.com/triton-lang/triton/blob/main/python/triton_kernels/triton_kernels/topk_details/_topk_forward.py\n",
        "        block_topk_val[i], vals_carry = (\n",
        "          jnp.where(v, vals_carry, block_topk_val[i]) for v in (mask, ~mask)\n",
        "        )\n",
        "        block_topk_index[i], index_carry = (\n",
        "          jnp.where(v, index_carry, block_topk_index[i]) for v in (mask, ~mask)\n",
        "        )\n",
        "    return (block_topk_val, block_topk_index)\n",
        "  return unrolled_fori_loop(logits.shape[-1] // num_blocks, while_body, (block_topk_val, block_topk_index), unroll=16\n",
        "  )\n",
        "\n",
        "# Pallas kernel scaffolding\n",
        "def topk_blockwise_superset_kernel(\n",
        "  logits_ref,\n",
        "  block_topm_val_ref,\n",
        "  block_topm_index_ref,\n",
        "  max_m_ref,\n",
        "  flag_ref,\n",
        "  num_blocks: int = 128,\n",
        "  k: int = 64,\n",
        "  m_schedule: tuple[int] | None = None,\n",
        "):\n",
        "  '''Compute blockwise top-m's until they contain global top-k.'''\n",
        "  ### Initialize refs\n",
        "  shape = block_topm_val_ref.shape\n",
        "  block_topm_val_ref[...] = jnp.full(shape, float('-inf'), dtype=logits_ref.dtype)\n",
        "  block_topm_index_ref[...] = jnp.full(shape, -1, dtype=jnp.int32)\n",
        "\n",
        "  block_token = logits_ref.shape[0]\n",
        "  for i in range(block_token):\n",
        "    # Worst case m = k\n",
        "    max_m_ref[pl.program_id(0) * block_token + i] = k\n",
        "\n",
        "  # flag for termination of while loop\n",
        "  flag_ref[0] = 0\n",
        "\n",
        "  ### Run increasing block topk, until sure overall topk present\n",
        "  if m_schedule is None:\n",
        "    m_schedule = (5, 8, 12)\n",
        "  # Ensure worst case of all k in one block is covered\n",
        "  m_schedule = (0,) + m_schedule + (k,)\n",
        "\n",
        "  for completed_m, m in zip(m_schedule, m_schedule[1:]):\n",
        "\n",
        "    @pl.when(flag_ref[0] == 0)\n",
        "    def _():\n",
        "      topk_vals, topk_indexs = blockwise_topk(\n",
        "        logits_ref,\n",
        "        # bf16, bf16 -> i1 mask not supported on v5e so we cast to f32\n",
        "        # TODO: check v6e, bf16 comparitor and make model specific\n",
        "        block_topk_val=[block_topm_val_ref[:,pl.dslice(i*num_blocks, num_blocks)].astype(jnp.float32) for i in range(m)],\n",
        "        block_topk_index=[block_topm_index_ref[:,pl.dslice(i*num_blocks, num_blocks)] for i in range(m)],\n",
        "        k=m,\n",
        "        start_k=completed_m,\n",
        "        mode='pallas',\n",
        "      )\n",
        "\n",
        "      for i in range(completed_m, m):\n",
        "        block_topm_val_ref[:,pl.dslice(i*num_blocks, num_blocks)] = topk_vals[i].astype(block_topm_val_ref.dtype)\n",
        "        block_topm_index_ref[:,pl.dslice(i*num_blocks,num_blocks)] = topk_indexs[i].astype(\n",
        "          block_topm_index_ref.dtype\n",
        "        )\n",
        "\n",
        "      # Stopping criterion check\n",
        "      # To find top-k values of a set, we can split into N subsets,\n",
        "      # and sort the largest, 2nd-largest, 3-rd largest, ..., m-th largest values for each subset\n",
        "      # When in the superset of top-(m-1) subsets there are more than k values\n",
        "      # larger (or equal than) the largest m'th largest value from the subsets\n",
        "      # then the top-(m-1) subsets must contain the top-k of the set.\n",
        "      # We run a schedule of m's until we have that full top-k found.\n",
        "      pivot_point = topk_vals[m - 1].max(-1, keepdims=True)\n",
        "      n_larger = (\n",
        "        sum([(v >= pivot_point) for v in topk_vals[: m - 1]])\n",
        "        .astype(jnp.float32)\n",
        "        .sum(-1)\n",
        "      )\n",
        "      # flag SMEM used to check if all searches terminated\n",
        "      flag_ref[0] = 0\n",
        "      for i in range(block_token):\n",
        "        blockwise_topm_contains_topk = n_larger[i] >= k\n",
        "        flag_ref[0] += blockwise_topm_contains_topk\n",
        "        # Store when the criteria was hit for each query\n",
        "        token_index = pl.program_id(0) * block_token + i\n",
        "        max_m = max_m_ref[token_index]\n",
        "        max_m_ref[token_index] = jnp.where(\n",
        "          blockwise_topm_contains_topk & (max_m == k), m - 1, max_m\n",
        "        )\n",
        "\n",
        "      # If not all terminated, reset the flag say we need to search deeper\n",
        "      @pl.when(flag_ref[0] != block_token)\n",
        "      def _():\n",
        "        flag_ref[0] = 0\n",
        "\n",
        "\n",
        "# Pallas function\n",
        "def topk_blockwise_superset_pallas(\n",
        "  logits, k, num_blocks=128, block_token=None, m_schedule=None\n",
        "):\n",
        "  num_tokens, vocab_size = logits.shape\n",
        "  if block_token is None:\n",
        "    block_token = min(32, num_tokens)\n",
        "  if num_tokens % block_token != 0:\n",
        "    raise ValueError('token block size must be a multiple of num tokens')\n",
        "\n",
        "  out_shape = (\n",
        "    jax.ShapeDtypeStruct((num_tokens, k*num_blocks), logits.dtype),\n",
        "    # uint16 fits vocab size of up to 2**16 * 128 = 8.4M. But not used to avoid unforseen issues.\n",
        "    jax.ShapeDtypeStruct((num_tokens, k*num_blocks), jnp.int32),\n",
        "    jax.ShapeDtypeStruct((num_tokens,), jnp.int32),\n",
        "    jax.ShapeDtypeStruct((1,), jnp.int32),  # scratch for termination flag\n",
        "  )\n",
        "  out_specs = jax.tree.map(\n",
        "    lambda _: pl.BlockSpec((block_token, k*num_blocks), lambda i: (i, 0)), out_shape[:2]\n",
        "  )\n",
        "  out_specs += (\n",
        "    pl.BlockSpec(memory_space=pltpu.SMEM),\n",
        "    pl.BlockSpec(memory_space=pltpu.SMEM),\n",
        "  )\n",
        "  return pl.pallas_call(\n",
        "    functools.partial(\n",
        "      topk_blockwise_superset_kernel,\n",
        "      k=k,\n",
        "      num_blocks=num_blocks,\n",
        "      m_schedule=m_schedule,\n",
        "    ),\n",
        "    in_specs=(pl.BlockSpec((block_token, vocab_size), lambda i: (i, 0)),),\n",
        "    out_shape=out_shape,\n",
        "    grid=(num_tokens // block_token),\n",
        "    out_specs=out_specs,\n",
        "    compiler_params=pltpu.TPUCompilerParams(vmem_limit_bytes=2**26),\n",
        "  )(logits)\n",
        "\n",
        "gather2d = jax.vmap(lambda x, y: x[y])\n",
        "def dense_gather_kernel(val_ref, index_ref, out_ref, num_lanes=128):\n",
        "  index = index_ref[...]\n",
        "  out = jnp.zeros(index_ref.shape, val_ref.dtype)\n",
        "  for i in range(0, val_ref.shape[1], num_lanes):\n",
        "    mask = (index >= i) & (index < i + num_lanes)\n",
        "    out = jnp.where(\n",
        "      mask,\n",
        "      gather2d(\n",
        "          val_ref[:, pl.dslice(i, num_lanes)],\n",
        "          index % num_lanes),\n",
        "      out\n",
        "    )\n",
        "  out_ref[...] = out.astype(out_ref.dtype)\n",
        "\n",
        "def dense_gather_pallas(val, index, out_dtype=None, num_lanes=128):\n",
        " if out_dtype is None:\n",
        "   out_dtype = val.dtype\n",
        " return pl.pallas_call(\n",
        "   dense_gather_kernel,\n",
        "   out_shape=jax.ShapeDtypeStruct(index.shape, out_dtype),\n",
        "   in_specs=(\n",
        "     pl.BlockSpec((8,val.shape[1]), lambda i: (i,0)),\n",
        "     pl.BlockSpec((8, num_lanes), lambda i:(i,0)),\n",
        "   ),\n",
        "   out_specs = pl.BlockSpec((8, num_lanes), lambda i: (i,0)),\n",
        "   grid = (val.shape[0] // 8,),\n",
        " )(val, index)\n",
        "\n",
        "\n",
        "def topk_on_filtered_subset(block_topm_val, block_topm_index, k):\n",
        "  num_blocks = 128\n",
        "  topk_logits, local_indices = jax.lax.top_k(\n",
        "    block_topm_val, k=k\n",
        "  )\n",
        "\n",
        "  block_topm_flat_index = (block_topm_index * num_blocks) + (jax.lax.broadcasted_iota(block_topm_index.dtype, block_topm_index.shape, 1) % num_blocks)\n",
        "  topk_flat_indices = dense_gather_pallas(\n",
        "    block_topm_flat_index,\n",
        "    jnp.concatenate([\n",
        "      local_indices,\n",
        "      jnp.zeros((local_indices.shape[0], num_blocks - k), local_indices.dtype)\n",
        "    ], axis=-1),\n",
        "  )[:,:k]\n",
        "  return topk_logits, topk_flat_indices\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "  jax.jit,\n",
        "  static_argnames=(\n",
        "    'k',\n",
        "    'num_blocks',\n",
        "    'm_stage1_schedule',\n",
        "    'm_stage2_schedule',\n",
        "    'block_token',\n",
        "  ),\n",
        ")\n",
        "def topk_optimized(\n",
        "  logits,\n",
        "  k: int = 64,\n",
        "  num_blocks: int = 128,\n",
        "  m_stage1_schedule: tuple[int] | None = None,\n",
        "  m_stage2_schedule: tuple[int] | None = None,\n",
        "  block_token: int | None = None,\n",
        "):\n",
        "  '''Fast implementation of jax.lax.top_k on TPUs.'''\n",
        "  if logits.ndim != 2:\n",
        "    raise ValueError('Expected 2D input')\n",
        "  block_topm_val, block_topm_index, termination_m, _ = topk_blockwise_superset_pallas(\n",
        "    logits,\n",
        "    k=k,\n",
        "    block_token=block_token,\n",
        "    m_schedule=m_stage1_schedule,\n",
        "    num_blocks=num_blocks,\n",
        "  )\n",
        "\n",
        "  # top-k the smallest number of values we can, by taking max m required\n",
        "  # such that all queries to have full top-k\n",
        "  # We compile for a range of shapes, then use jax.lax.cond to run just one.\n",
        "  # in practice 8 nearly always sufficient\n",
        "  if m_stage2_schedule is None:\n",
        "    m_init = 8\n",
        "    m_stage2_schedule = [\n",
        "      m_init * (2**i) for i in range(int(math.log2(k // m_init)) + 1)\n",
        "    ]\n",
        "  # Guarantee all cases covered\n",
        "  m_stage2_schedule = (-1,) + tuple(m_stage2_schedule) + (k,)\n",
        "\n",
        "  # Buffer for output to be written in to\n",
        "  topk_logits, topk_flat_indices = jax.tree.map(\n",
        "    jnp.zeros_like,\n",
        "    topk_on_filtered_subset(block_topm_val, block_topm_index, k=k),\n",
        "  )\n",
        "  max_m = termination_m.max()\n",
        "  for lower_m, upper_m in zip(m_stage2_schedule, m_stage2_schedule[1:]):\n",
        "    topk_logits, topk_flat_indices = jax.lax.cond(\n",
        "      (max_m > lower_m) & (max_m <= upper_m),\n",
        "      lambda *args: topk_on_filtered_subset(\n",
        "        block_topm_val=block_topm_val[:,:num_blocks*upper_m],\n",
        "        block_topm_index=block_topm_index[:num_blocks*upper_m],\n",
        "        k=k,\n",
        "      ),\n",
        "      lambda *args: args,\n",
        "      topk_logits,\n",
        "      topk_flat_indices,\n",
        "    )\n",
        "  return topk_logits, topk_flat_indices\n",
        "\n",
        "import functools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "k = 64\n",
        "num_queries = 32\n",
        "vocab_size = 201088\n",
        "hidden_dim = 2880\n",
        "\n",
        "logit_key, key_act, key_weight = jax.random.split(jax.random.key(0), 3)\n",
        "x = jax.random.normal(key_act, (num_queries, hidden_dim), dtype=jnp.bfloat16)\n",
        "w = jax.random.normal(key_weight, (hidden_dim, vocab_size), dtype=jnp.bfloat16)\n",
        "logits = jax.random.normal(key_weight, (num_queries, vocab_size), dtype=jnp.float32).astype(jnp.bfloat16)\n",
        "\n",
        "topk_xla = jax.jit(jax.lax.top_k, static_argnames=('k',))\n",
        "approx_topk_xla = jax.jit(jax.lax.approx_max_k, static_argnames=('k',))\n",
        "\n",
        "@jax.jit\n",
        "@functools.partial(jax.vmap, in_axes=(0, None))\n",
        "def matmul_and_topk_xla(x, w, k=k):\n",
        "  logits = (x @ w)\n",
        "  return jax.lax.top_k(logits, k)\n",
        "\n",
        "def run():\n",
        "  # reference runtimes\n",
        "  #o = jax.block_until_ready(x @ w)\n",
        "  #jax.block_until_ready(matmul_and_topk_xla(x, w))\n",
        "  jax.block_until_ready(topk_xla(logits, k=k))\n",
        "\n",
        "  # Optimized tpu run on random logits\n",
        "  jax.block_until_ready(topk_optimized(logits, k=k))\n",
        "\n",
        "  # Not exact. Runtime varies with recall, here run with default 0.95\n",
        "  jax.block_until_ready(approx_topk_xla(logits, k=k))\n",
        "\n",
        "\n",
        "\n",
        "run()\n",
        "with jax.profiler.trace('/content/'):\n",
        "  run()\n",
        "\n",
        "\n",
        "import gzip\n",
        "from glob import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "path = sorted(glob('/content/plugins/profile/*/**.json.gz'), key=os.path.getmtime)[-1]\n",
        "print(path)\n",
        "trace = json.load(gzip.open(path))\n",
        "df = pd.DataFrame(trace['traceEvents'])\n",
        "df = df[~df.name.isna()]\n",
        "df[df.name.str.contains('jit_topk_optimized')].iloc[0].dur\n",
        "\n"
      ],
      "metadata": {
        "id": "VAOC7EkVOc_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58c0b30-6340-48e0-fee9-ab373aa2dcd0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:82: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/plugins/profile/2025_10_02_00_36_03/c718fa664a9c.trace.json.gz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(76.451172)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NoU56QUyC07c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.lax.top_k(logits, k), topk_optimized(logits, k)"
      ],
      "metadata": {
        "id": "lsKxCZp0WZ2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b22fe4-5fb2-42f5-b0f3-6fda0f6dad49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Array([[4.4375, 4.28125, 4.1875, ..., 3.42188, 3.40625, 3.40625],\n",
              "         [4.59375, 4.46875, 4.25, ..., 3.45312, 3.4375, 3.4375],\n",
              "         [4.65625, 4.53125, 4.15625, ..., 3.45312, 3.45312, 3.4375],\n",
              "         ...,\n",
              "         [4.5625, 4.5, 4.34375, ..., 3.39062, 3.39062, 3.39062],\n",
              "         [4.34375, 4.3125, 4.09375, ..., 3.4375, 3.42188, 3.42188],\n",
              "         [4.53125, 4.40625, 4.0625, ..., 3.40625, 3.40625, 3.40625]],      dtype=bfloat16),\n",
              "  Array([[101171,  58512,  64088, ..., 141062,  15207, 116690],\n",
              "         [ 39735,  63692, 112073, ..., 186850,  22206,  70731],\n",
              "         [192142,  55046, 153857, ..., 137158, 142069,   7672],\n",
              "         ...,\n",
              "         [125137,  79273,  25911, ...,  20426,  66520,  99947],\n",
              "         [ 56456, 126343,  96079, ..., 100138,  28483, 162640],\n",
              "         [ 22603, 114276,  15680, ...,  89922,  91875,  92696]],      dtype=int32)],\n",
              " (Array([[4.4375, 4.28125, 4.1875, ..., 3.42188, 3.40625, 3.40625],\n",
              "         [4.59375, 4.46875, 4.25, ..., 3.45312, 3.4375, 3.4375],\n",
              "         [4.65625, 4.53125, 4.15625, ..., 3.45312, 3.45312, 3.4375],\n",
              "         ...,\n",
              "         [4.5625, 4.5, 4.34375, ..., 3.39062, 3.39062, 3.39062],\n",
              "         [4.34375, 4.3125, 4.09375, ..., 3.4375, 3.42188, 3.42188],\n",
              "         [4.53125, 4.40625, 4.0625, ..., 3.40625, 3.40625, 3.40625]],      dtype=bfloat16),\n",
              "  Array([[101171,  58512,  64088, ..., 141062,  15207, 140672],\n",
              "         [ 39735,  63692, 112073, ...,  63073,  22206,  70731],\n",
              "         [192142,  55046, 153857, ..., 142069, 137158,  87434],\n",
              "         ...,\n",
              "         [125137,  79273,  25911, ...,  20426, 152018,  66520],\n",
              "         [ 56456, 126343,  96079, ..., 100138,  28483, 162640],\n",
              "         [ 22603, 114276,  15680, ...,  89922,  91875, 192488]],      dtype=int32)))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}
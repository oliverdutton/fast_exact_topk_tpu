{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\nimport functools\nimport math\n\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental import pallas as pl\nfrom jax.experimental.pallas import tpu as pltpu\n\ndef unrolled_fori_loop(length, body_fn, init, unroll):\n def unrolled_body(i, c):\n   i *= unroll\n   for j in range(unroll):\n     c = body_fn(i + j, c)\n   return c\n carry = jax.lax.fori_loop(0, length // unroll, unrolled_body, init)\n for j in range(length % unroll):\n   carry = body_fn((length // unroll) * unroll + j, carry)\n return carry\n \ndef blockwise_topk(\n  logits,\n  k,\n  block_topk_val=None,\n  block_topk_index=None,\n  start_k=0,\n  num_blocks=128,\n  mode='jax',\n):\n  '''Compute blockwise top-k.'''\n  ntokens = logits.shape[0]\n\n  if start_k != 0 and (block_topk_val is None or block_topk_index is None):\n    raise ValueError(\n      'start_k must be 0, unless precomputed top-(start_k) in a buffer is provided in block_topk_val and block_topk_index'\n    )\n  if mode == 'jax':\n    block_topk_val = [\n      jnp.full((ntokens, num_blocks), float('-inf'), dtype=logits.dtype)\n      for i in range(k)\n    ]\n    # TODO?: Could use uint16 when vocab size < 4M if hardware supports\n    block_topk_index = [\n      jnp.full((ntokens, num_blocks), -1, dtype=jnp.int32) for i in range(k)\n    ]\n  elif mode == 'pallas':\n    if block_topk_val is None or block_topk_index is None:\n      raise ValueError(\n        'Pass through of block_topk_val and tok_index expected for pallas topk.'\n      )\n\n  def while_body(i, while_carry):\n    block_topk_val, block_topk_index = while_carry\n\n    if mode == 'pallas':\n      vals_carry = logits[..., pl.dslice(num_blocks * i, num_blocks)]\n    elif mode == 'jax':\n      vals_carry = jax.lax.dynamic_slice_in_dim(\n        logits, num_blocks * i, num_blocks, axis=1\n      )\n    else:\n      raise ValueError(\n        'mode must be either `pallas` and a memory ref or `jax` and an array'\n      )\n\n    index_carry = jnp.full((ntokens, num_blocks), i, jnp.int32)\n\n    for i in range(k):\n      if i < start_k:\n        # Nothing will be exchanged into the completed block topk, we just need\n        # to invalidate it from flowing downward. So we check if it's already\n        # found and invalidate if so.\n        vals_carry = jnp.where(\n          index_carry == block_topk_index[i], float('-inf'), vals_carry\n        )\n      else:\n        # Sinking sort\n        mask = vals_carry > block_topk_val[i]\n        # TODO: Consider packing bfloat16 val and uint16 index into single uint32\n        # and packed sort as in\n        # https://github.com/triton-lang/triton/blob/main/python/triton_kernels/triton_kernels/topk_details/_topk_forward.py\n        block_topk_val[i], vals_carry = (\n          jnp.where(v, vals_carry, block_topk_val[i]) for v in (mask, ~mask)\n        )\n        block_topk_index[i], index_carry = (\n          jnp.where(v, index_carry, block_topk_index[i]) for v in (mask, ~mask)\n        )\n    return (block_topk_val, block_topk_index)\n  return unrolled_fori_loop(logits.shape[-1] // num_blocks, while_body, (block_topk_val, block_topk_index), unroll=8\n  )\n  #(block_topk_val, block_topk_index) = jax.lax.fori_loop(\n  #0, logits.shape[-1] // num_blocks, while_body, (block_topk_val, block_topk_index)\n  #)\n  #return block_topk_val, block_topk_index\n\n\n# Pallas kernel scaffolding\ndef topk_blockwise_superset_kernel(\n  logits_ref,\n  block_topm_val_refs,\n  block_topm_index_refs,\n  max_m_ref,\n  flag_ref,\n  num_blocks: int = 128,\n  k: int = 64,\n  m_schedule: tuple[int] | None = None,\n):\n  '''Compute blockwise top-m's until they contain global top-k.'''\n  ### Initialize refs\n  shape = block_topm_val_refs[0].shape\n  for i in range(len(block_topm_val_refs)):\n    block_topm_val_refs[i][...] = jnp.full(shape, float('-inf'), dtype=logits_ref.dtype)\n    block_topm_index_refs[i][...] = jnp.full(shape, -1, dtype=jnp.int32)\n\n  block_token = logits_ref.shape[0]\n  for i in range(block_token):\n    # Worst case m = k\n    max_m_ref[pl.program_id(0) * block_token + i] = k\n\n  # flag for termination of while loop\n  flag_ref[0] = 0\n\n  ### Run increasing block topk, until sure overall topk present\n  if m_schedule is None:\n    m_schedule = (5, 8, 12)\n  # Ensure worst case of all k in one block is covered\n  m_schedule = (0,) + m_schedule + (k,)\n\n  for completed_m, m in zip(m_schedule, m_schedule[1:]):\n\n    @pl.when(flag_ref[0] == 0)\n    def _():\n      topk_vals, topk_indexs = blockwise_topk(\n        logits_ref,\n        # bf16, bf16 -> i1 mask not supported on v5e so we cast to f32\n        # TODO: check v6e, bf16 comparitor and make model specific\n        block_topk_val=jax.tree.map(\n          lambda ref: ref[...].astype(jnp.float32), block_topm_val_refs\n        ),\n        block_topk_index=jax.tree.map(lambda ref: ref[...], block_topm_index_refs),\n        k=m,\n        start_k=completed_m,\n        mode='pallas',\n      )\n\n      for i in range(completed_m, m):\n        block_topm_val_refs[i][...] = topk_vals[i].astype(block_topm_val_refs[i].dtype)\n        block_topm_index_refs[i][...] = topk_indexs[i].astype(\n          block_topm_index_refs[i].dtype\n        )\n\n      # Stopping criterion check\n      # To find top-k values of a set, we can split into N subsets,\n      # and sort the largest, 2nd-largest, 3-rd largest, ..., m-th largest values for each subset\n      # When in the superset of top-(m-1) subsets there are more than k values\n      # larger (or equal than) the largest m'th largest value from the subsets\n      # then the top-(m-1) subsets must contain the top-k of the set.\n      # We run a schedule of m's until we have that full top-k found.\n      pivot_point = topk_vals[m - 1].max(-1, keepdims=True)\n      n_larger = (\n        sum([(v >= pivot_point) for v in topk_vals[: m - 1]])\n        .astype(jnp.float32)\n        .sum(-1)\n      )\n      # flag SMEM used to check if all searches terminated\n      flag_ref[0] = 0\n      for i in range(block_token):\n        blockwise_topm_contains_topk = n_larger[i] >= k\n        flag_ref[0] += blockwise_topm_contains_topk\n        # Store when the criteria was hit for each query\n        token_index = pl.program_id(0) * block_token + i\n        max_m = max_m_ref[token_index]\n        max_m_ref[token_index] = jnp.where(\n          blockwise_topm_contains_topk & (max_m == k), m - 1, max_m\n        )\n\n      # If not all terminated, reset the flag say we need to search deeper\n      @pl.when(flag_ref[0] != block_token)\n      def _():\n        flag_ref[0] = 0\n\n\n# Pallas function\ndef topk_blockwise_superset_pallas(\n  logits, k, num_blocks=128, block_token=None, m_schedule=None\n):\n  num_tokens, vocab_size = logits.shape\n  if block_token is None:\n    block_token = min(32, num_tokens)\n  if num_tokens % block_token != 0:\n    raise ValueError('token block size must be a multiple of num tokens')\n\n  out_shape = (\n    [jax.ShapeDtypeStruct((num_tokens, num_blocks), logits.dtype) for i in range(k)],\n    # uint16 fits vocab size of up to 2**16 * 128 = 8.4M. But not used to avoid unforseen issues.\n    [jax.ShapeDtypeStruct((num_tokens, num_blocks), jnp.int32) for i in range(k)],\n    jax.ShapeDtypeStruct((num_tokens,), jnp.int32),\n    jax.ShapeDtypeStruct((1,), jnp.int32),  # scratch for termination flag\n  )\n  out_specs = jax.tree.map(\n    lambda _: pl.BlockSpec((block_token, num_blocks), lambda i: (i, 0)), out_shape[:2]\n  )\n  out_specs += (\n    pl.BlockSpec(memory_space=pltpu.SMEM),\n    pl.BlockSpec(memory_space=pltpu.SMEM),\n  )\n  return pl.pallas_call(\n    functools.partial(\n      topk_blockwise_superset_kernel,\n      k=k,\n      num_blocks=num_blocks,\n      m_schedule=m_schedule,\n    ),\n    in_specs=(pl.BlockSpec((block_token, vocab_size), lambda i: (i, 0)),),\n    out_shape=out_shape,\n    grid=(num_tokens // block_token),\n    out_specs=out_specs,\n    compiler_params=pltpu.TPUCompilerParams(vmem_limit_bytes=2**26),\n  )(logits)\n\ngather2d = jax.vmap(lambda x, y: x[y])\ndef dense_gather_kernel(val_ref, index_ref, out_ref, num_lanes=128):\n  index = index_ref[...]\n  out = jnp.zeros(index_ref.shape, val_ref.dtype)\n  for i in range(0, val_ref.shape[1], num_lanes):\n    mask = (index >= i) & (index < i + num_lanes)\n    out = jnp.where(\n      mask,\n      gather2d(val_ref[:, pl.dslice(i, num_blocks)], index % num_blocks),\n      out\n    )\n  out_ref[...] = out.astype(out_ref.dtype)\n\ndef dense_gather_pallas(val, index, out_dtype=None):\n if out_dtype is None:\n   out_dtype = val.dtype\n return pl.pallas_call(\n   dense_gather_kernel,\n   out_shape=jax.ShapeDtypeStruct(index.shape, out_dtype),\n )\n\n\ndef topk_on_filtered_subset(block_topm_val, block_topm_index, k):\n  num_blocks = block_topm_val[0].shape[-1]\n  topk_logits, local_indices = jax.lax.top_k(\n    jnp.concatenate(block_topm_val, axis=-1), k=k\n  )\n\n  @jax.vmap\n  def unravel_indices(local_indices, block_topm_index):\n    m, col = jnp.unravel_index(local_indices, (k, num_blocks))\n    row = jnp.stack(block_topm_index)[m, col]\n    flat_index = row * num_blocks + col\n    return flat_index\n    \n  block_topm_flat_index = jnp.concatenate(\n  [v * 128 + jax.lax.broadcasted_iota(v.dtype, v.shape, 1) \n  for v in block_topm_index], axis=-1)\n  topk_flat_indices = dense_gather_pallas(\n    block_topm_flat_index,\n    jnp.concatenate([\n      local_indices,\n      jnp.zeros((local_indices.shape[0], 128 - k), local_indices.dtype)\n    ], axis=-1),\n  )[:,:k]\n\n  #topk_flat_indices = unravel_indices(local_indices, block_topm_index)\n  return topk_logits, topk_flat_indices\n\n\n@functools.partial(\n  jax.jit,\n  static_argnames=(\n    'k',\n    'num_blocks',\n    'm_stage1_schedule',\n    'm_stage2_schedule',\n    'block_token',\n  ),\n)\ndef topk_optimized(\n  logits,\n  k: int = 64,\n  num_blocks: int = 128,\n  m_stage1_schedule: tuple[int] | None = None,\n  m_stage2_schedule: tuple[int] | None = None,\n  block_token: int | None = None,\n):\n  '''Fast implementation of jax.lax.top_k on TPUs.'''\n  if logits.ndim != 2:\n    raise ValueError('Expected 2D input')\n  block_topm_val, block_topm_index, termination_m, _ = topk_blockwise_superset_pallas(\n    logits,\n    k=k,\n    block_token=block_token,\n    m_schedule=m_stage1_schedule,\n    num_blocks=num_blocks,\n  )\n\n  # top-k the smallest number of values we can, by taking max m required\n  # such that all queries to have full top-k\n  # We compile for a range of shapes, then use jax.lax.cond to run just one.\n  # in practice 8 nearly always sufficient\n  if m_stage2_schedule is None:\n    m_init = 8\n    m_stage2_schedule = [\n      m_init * (2**i) for i in range(int(math.log2(k // m_init)) + 1)\n    ]\n  # Guarantee all cases covered\n  m_stage2_schedule = (-1,) + tuple(m_stage2_schedule) + (k,)\n\n  # Buffer for output to be written in to\n  topk_logits, topk_flat_indices = jax.tree.map(\n    jnp.zeros_like,\n    topk_on_filtered_subset(block_topm_val[:1], block_topm_index[:1], k=k),\n  )\n  max_m = termination_m.max()\n  for lower_m, upper_m in zip(m_stage2_schedule, m_stage2_schedule[1:]):\n    topk_logits, topk_flat_indices = jax.lax.cond(\n      (max_m > lower_m) & (max_m <= upper_m),\n      lambda *args: topk_on_filtered_subset(\n        block_topm_val=block_topm_val[:upper_m],\n        block_topm_index=block_topm_index[:upper_m],\n        k=k,\n      ),\n      lambda *args: args,\n      topk_logits,\n      topk_flat_indices,\n    )\n  return topk_logits, topk_flat_indices\n  \nimport functools\nimport jax\nimport jax.numpy as jnp\n\nk = 64\nnum_queries = 32\nvocab_size = 201088\nhidden_dim = 2880\n\nlogit_key, key_act, key_weight = jax.random.split(jax.random.key(0), 3)\nx = jax.random.normal(key_act, (num_queries, hidden_dim), dtype=jnp.bfloat16)\nw = jax.random.normal(key_weight, (hidden_dim, vocab_size), dtype=jnp.bfloat16)\nlogits = jax.random.normal(key_weight, (num_queries, vocab_size), dtype=jnp.float32).astype(jnp.bfloat16)\n\ntopk_xla = jax.jit(jax.lax.top_k, static_argnames=('k',))\napprox_topk_xla = jax.jit(jax.lax.approx_max_k, static_argnames=('k',))\n\n@jax.jit\n@functools.partial(jax.vmap, in_axes=(0, None))\ndef matmul_and_topk_xla(x, w, k=k):\n  logits = (x @ w)\n  return jax.lax.top_k(logits, k)\n\ndef run():\n  # reference runtimes\n  #o = jax.block_until_ready(x @ w)\n  #jax.block_until_ready(matmul_and_topk_xla(x, w))\n  jax.block_until_ready(topk_xla(logits, k=k))\n\n  # Optimized tpu run on random logits\n  jax.block_until_ready(topk_optimized(logits, k=k))\n\n  # Not exact. Runtime varies with recall, here run with default 0.95\n  jax.block_until_ready(approx_topk_xla(logits, k=k))\n\n\n\nrun()\nwith jax.profiler.trace('/content/'):\n  run()\n\n\nimport gzip\nfrom glob import glob\nimport json\nimport pandas as pd\npath = glob('/content/plugins/profile/*/**.json.gz')[0]\ntrace = json.load(gzip.open(path))\ndf = pd.DataFrame(trace['traceEvents'])\ndf = df[~df.name.isna()]\ndf[df.name.str.contains('jit_topk_optimized')].iloc[0].dur\n\n"

      ],
      "metadata": {
        "id": "VAOC7EkVOc_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsKxCZp0WZ2K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
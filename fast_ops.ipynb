{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oliverdutton/fast_exact_topk_tpu/blob/unrolled/fast_ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c38e190-d04b-44d2-beee-215a859b02d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c38e190-d04b-44d2-beee-215a859b02d2",
        "outputId": "ae7244a2-a007-431f-c728-256e563694d4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:82: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y shape, dtype  (32, 8192) float32\n",
            "                                                  name         dur\n",
            "191                       jit_sort(281544927581784284)  170.846172\n",
            "192  jit_sort_pallas_vmem_efficient(346995495189801...   45.458594\n",
            "193                   jit_argsort(9739793870242214563)  170.903750\n",
            "194  jit_sort_pallas_vmem_efficient(145469671854719...   82.052422\n",
            "195                   jit_add_one(8327475351217128358)    3.555078\n",
            "sort  (32, 8192) float32\n",
            "match:  1.0\n",
            "argsort  (32, 8192) float32\n",
            "match:  0.9994812\n",
            "y shape, dtype  (32, 32768) float32\n",
            "                                                  name         dur\n",
            "192                      jit_sort(8979964914546130889)  925.918672\n",
            "193  jit_sort_pallas_vmem_efficient(352534473703344...  240.802656\n",
            "194                  jit_argsort(13822368489657809638)  925.912656\n",
            "195  jit_sort_pallas_vmem_efficient(567293098151402...  426.722656\n",
            "196                   jit_add_one(4238132308555217233)   12.856250\n",
            "sort  (32, 32768) float32\n",
            "match:  1.0\n",
            "argsort  (32, 32768) float32\n",
            "match:  0.99795055\n",
            "y shape, dtype  (32, 131072) float32\n",
            "                                                  name          dur\n",
            "192                     jit_sort(10743320738726565617)  5058.476406\n",
            "193  jit_sort_pallas_vmem_efficient(817840973158341...  1211.986406\n",
            "194                  jit_argsort(17836659088745114208)  5056.177422\n",
            "195  jit_sort_pallas_vmem_efficient(805309699584518...  2272.283594\n",
            "196                   jit_add_one(4928732959024541938)    51.055000\n",
            "sort  (32, 131072) float32\n",
            "match:  1.0\n",
            "argsort  (32, 131072) float32\n",
            "match:  0.99219394\n",
            "y shape, dtype  (32, 1048576) float32\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Optimized Bitonic Sort, Top-K and Gather Implementation for TPU using JAX/Pallas.\n",
        "\n",
        "This module provides efficient sorting and top-k operations optimized for TPU hardware,\n",
        "utilizing bitonic sorting algorithms with tile-based processing.\n",
        "\"\"\"\n",
        "\n",
        "import functools\n",
        "from functools import lru_cache\n",
        "import math\n",
        "import gzip\n",
        "import json\n",
        "import os\n",
        "from glob import glob\n",
        "from collections.abc import Callable\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, lax\n",
        "from jax.experimental import pallas as pl\n",
        "from jax.experimental import checkify\n",
        "from jax.experimental.pallas import tpu as pltpu\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Hardware Constants\n",
        "# ============================================================================\n",
        "\n",
        "NUM_SUBLANES = 8\n",
        "NUM_LANES = 128\n",
        "LOG_LANES = 7  # log2(NUM_LANES)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Performance Statistics\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_performance_stats(num_stages: int, num_tokens: int, time_us: float):\n",
        "  \"\"\"Calculate and print performance statistics for sorting operations.\"\"\"\n",
        "  num_tiles = ((2**num_stages) // NUM_LANES)* (num_tokens // NUM_SUBLANES)\n",
        "  stage_sequence = (1, 2, 3, 4, 5, 6)\n",
        "  permutes_per_tile = (\n",
        "      sum(stage_sequence[:num_stages]) +\n",
        "      7 * max(0, num_stages - len(stage_sequence))\n",
        "  )\n",
        "\n",
        "  total_permutes = permutes_per_tile * num_tiles\n",
        "  total_cycles = (time_us * 1e-6) * (1.75e9)\n",
        "  cycles_per_permute = total_cycles / total_permutes\n",
        "\n",
        "  print(f\"Permutes: {total_permutes}, Cycles: {total_cycles}, \"\n",
        "        f\"Cycles/Permute: {cycles_per_permute}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Utility Functions\n",
        "# ============================================================================\n",
        "\n",
        "@lru_cache\n",
        "def _log2(value: int) -> int:\n",
        "  \"\"\"Calculate log base 2 of an integer.\"\"\"\n",
        "  log_result = 0\n",
        "  n = value\n",
        "  while n > 1:\n",
        "    n = n // 2\n",
        "    log_result += 1\n",
        "  return log_result\n",
        "\n",
        "# colab version of JAX lacks pl.loop, so reimplement\n",
        "def loop(\n",
        "    lower: jax.typing.ArrayLike,\n",
        "    upper: jax.typing.ArrayLike,\n",
        "    *,\n",
        "    step: jax.typing.ArrayLike = 1,\n",
        "    unroll: int | bool | None = None,\n",
        ") -> Callable[[Callable[[jax.Array], None]], None]:\n",
        "  \"\"\"Returns a decorator that calls the decorated function in a loop.\"\"\"\n",
        "  zero: jax.typing.ArrayLike\n",
        "  if not all(map(lambda v: type(v) == int, (lower, upper, step))):\n",
        "    idx_type = jnp.result_type(lower, upper, step)\n",
        "    lower = jax.lax.convert_element_type(lower, idx_type)\n",
        "    upper = jax.lax.convert_element_type(upper, idx_type)\n",
        "    step = jax.lax.convert_element_type(step, idx_type)\n",
        "    zero = jnp.array(0, dtype=idx_type)\n",
        "  else:\n",
        "    zero = 0\n",
        "\n",
        "  def decorator(body_fn):\n",
        "    jax.lax.fori_loop(\n",
        "        zero,\n",
        "        pl.cdiv(upper - lower, step),\n",
        "        lambda idx, _: body_fn(lower + idx * step),\n",
        "        init_val=None,\n",
        "        unroll=unroll,\n",
        "    )\n",
        "  return decorator\n",
        "\n",
        "\n",
        "# JAX lowering for Pallas doesnt support integer unroll\n",
        "def unrolled_fori_loop(length: int, body_fn, init_val, unroll: int):\n",
        "  \"\"\"Execute a for loop with manual unrolling for better performance.\"\"\"\n",
        "  unroll = min(length, unroll)\n",
        "\n",
        "  def unrolled_body(i, carry):\n",
        "    i *= unroll\n",
        "    for j in range(unroll):\n",
        "      carry = body_fn(i + j, carry)\n",
        "    return carry\n",
        "\n",
        "  carry = jax.lax.fori_loop(0, length // unroll, unrolled_body, init_val)\n",
        "  for j in range(length % unroll):\n",
        "    carry = body_fn((length // unroll) * unroll + j, carry)\n",
        "  return carry\n",
        "\n",
        "\n",
        "gather_2d = jax.vmap(lambda x, index: x[index])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Value Packing/Unpacking for Index Preservation\n",
        "# ============================================================================\n",
        "\n",
        "def pack_value_with_index(val, index):\n",
        "  \"\"\"\n",
        "  Pack bfloat16 value and int32 index into a single float32.\n",
        "  This allows sorting while preserving original indices.\n",
        "  \"\"\"\n",
        "  assert index.dtype == jnp.int32\n",
        "  # BF16 values in F32 have empty lower 16 bits in mantissa where we pack the index\n",
        "  return lax.bitcast_convert_type(\n",
        "      lax.bitcast_convert_type(val.astype(jnp.float32), jnp.int32) | index,\n",
        "      jnp.float32,\n",
        "  )\n",
        "\n",
        "\n",
        "def unpack_value_and_index(packed):\n",
        "  \"\"\"Extract the original value and index from packed representation.\"\"\"\n",
        "  val = lax.bitcast_convert_type(\n",
        "      lax.bitcast_convert_type(packed, jnp.int32) & ~0xFFFF, jnp.float32\n",
        "  ).astype(jnp.bfloat16)\n",
        "  index = lax.bitcast_convert_type(packed, jnp.int32) & 0xFFFF\n",
        "  return val, index\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Tile Management\n",
        "# ============================================================================\n",
        "\n",
        "def split_array_to_tiles(array_ref, use_dslice=True):\n",
        "  \"\"\"Split a 2D array into a flat list of tiles.\"\"\"\n",
        "  num_rows, num_cols = array_ref.shape\n",
        "  tile_rows = num_rows // NUM_SUBLANES\n",
        "  tile_cols = num_cols // NUM_LANES\n",
        "\n",
        "  tiles = []\n",
        "  for row in range(tile_rows):\n",
        "    for col in range(tile_cols):\n",
        "      if use_dslice:\n",
        "        tile = array_ref[\n",
        "            pl.dslice(row * NUM_SUBLANES, NUM_SUBLANES),\n",
        "            pl.dslice(col * NUM_LANES, NUM_LANES)\n",
        "        ]\n",
        "      else:\n",
        "        tile = array_ref[\n",
        "            row * NUM_SUBLANES: (row + 1) * NUM_SUBLANES,\n",
        "            col * NUM_LANES: (col + 1) * NUM_LANES,\n",
        "        ]\n",
        "      tiles.append(tile)\n",
        "  return tiles\n",
        "\n",
        "\n",
        "def join_tiles_to_array(target_shape, tiles):\n",
        "  \"\"\"Reconstruct a 2D array from a flat list of tiles.\"\"\"\n",
        "  num_rows, num_cols = target_shape\n",
        "  tile_rows, tile_cols = tiles[0].shape\n",
        "  grid_rows = num_rows // tile_rows\n",
        "  grid_cols = num_cols // tile_cols\n",
        "\n",
        "  rows = []\n",
        "  for i in range(pl.cdiv(len(tiles), grid_cols)):\n",
        "    row_tiles = tiles[i * grid_cols: (i + 1) * grid_cols]\n",
        "    rows.append(jnp.concatenate(row_tiles, axis=-1))\n",
        "\n",
        "  return jnp.concatenate(rows, axis=-2)\n",
        "\n",
        "def create_bit_indicator(bit_position: int, index=None):\n",
        "  \"\"\"Create a boolean mask indicating which elements have a specific bit set.\"\"\"\n",
        "  if index is None:\n",
        "    index = lax.broadcasted_iota(jnp.int32, (NUM_SUBLANES, NUM_LANES), 1)\n",
        "  return (index & (1 << bit_position)) > 0\n",
        "\n",
        "\n",
        "def compute_crosstile_substage(\n",
        "    array_ref,\n",
        "    substage: int,\n",
        "    stage: int,\n",
        "    sort_order: int,\n",
        "    aux_refs=(),\n",
        "    unroll: int = 16,\n",
        "    concat_before_writeout: bool = False,\n",
        "    read_then_split: bool = False,\n",
        "    dim1_offset: int = 0,\n",
        "    dim1_length: int = None\n",
        "):\n",
        "  \"\"\"\n",
        "  Perform a substage of sort involving comparisons between tiles\n",
        "\n",
        "  Args:\n",
        "      array_ref: Reference to array being sorted\n",
        "      aux_ref: array to be sorted according to value in array_ref (e.g. array indices)\n",
        "      substage: Current substage within the stage\n",
        "      stage: Current sorting stage\n",
        "      sort_order: 0=ascending, 1=descending, 2=bitonic\n",
        "      unroll: Loop unrolling factor\n",
        "  \"\"\"\n",
        "  assert (unroll % 2) == 0, 'Static sort order requires even unroll factor'\n",
        "\n",
        "  num_pairs = array_ref.shape[-1] // 2 ** (substage + 1)\n",
        "  unroll = min(unroll, num_pairs)\n",
        "\n",
        "  if dim1_length is not None:\n",
        "    raise NotImplementedError(\"Dynamic array size not yet supported\")\n",
        "\n",
        "  @loop(0, pl.cdiv(num_pairs, unroll))\n",
        "  def process_pairs(loop_idx):\n",
        "    outputs = []\n",
        "    aux_outputs = [[] for _ in aux_refs]\n",
        "    is_descending = sort_order == 1\n",
        "    pair_length = 2 ** (substage + 1)\n",
        "    slice_length = unroll * pair_length\n",
        "    array_slice, *aux_slices = (ref.at[:, pl.dslice(loop_idx * slice_length, slice_length)] for ref in (array_ref, *aux_refs))\n",
        "\n",
        "    data, *aux_datas = (v[...] if read_then_split else v for v in (array_slice, *aux_slices))\n",
        "\n",
        "    for i in range(unroll):\n",
        "      pair_offset = (loop_idx * unroll + i) * pair_length\n",
        "      half_length = 2 ** substage\n",
        "\n",
        "      # Slice subarrays to be compared\n",
        "      left, *aux_lefts  = (v[:, i * pair_length: i * pair_length + half_length] for v in (data, *aux_datas))\n",
        "      right, *aux_rights = (v[:, i * pair_length + half_length: i * pair_length + 2 * half_length] for v in (data, *aux_datas))\n",
        "\n",
        "      # Determine the swap mask based on the main array's values\n",
        "      if sort_order != 2:\n",
        "        mask = (left > right) if is_descending else (left < right)\n",
        "      else:\n",
        "        is_descending_bit = create_bit_indicator(stage, dim1_offset + pair_offset)\n",
        "        mask = jnp.bitwise_xor(is_descending_bit, left < right)\n",
        "\n",
        "      # Store the sorted pairs for the main array\n",
        "      outputs.append([jnp.where(m, left, right) for m in (mask, ~mask)])\n",
        "\n",
        "      # Apply the *same mask* to the auxiliary arrays and store the results\n",
        "      for i_aux, (l, r) in enumerate(zip(aux_lefts, aux_rights)):\n",
        "        aux_outputs[i_aux].append([jnp.where(m, l, r) for m in (mask, ~mask)])\n",
        "\n",
        "    if concat_before_writeout:\n",
        "      # Concatenate all sorted pairs and write the entire slice back at once\n",
        "      array_slice[...] = jnp.concatenate(jax.tree.leaves(outputs), axis=-1)\n",
        "      for i_aux, aux_slice in enumerate(aux_slices):\n",
        "        aux_slice[...] = jnp.concatenate(jax.tree.leaves(aux_outputs[i_aux]), axis=-1)\n",
        "    else:\n",
        "      # Write back each sorted pair individually\n",
        "      for i in range(unroll):\n",
        "        pair_offset = (loop_idx * unroll + i) * pair_length\n",
        "        half_length = 2 ** substage\n",
        "\n",
        "        # Write back main array pairs\n",
        "        array_ref[:, pl.dslice(pl.multiple_of(pair_offset, half_length), half_length)] = outputs[i][0]\n",
        "        array_ref[:, pl.dslice(pl.multiple_of(pair_offset + half_length, half_length), half_length)] = outputs[i][1]\n",
        "\n",
        "        # Write back auxiliary array pairs\n",
        "        for i_aux, aux_ref_i in enumerate(aux_refs):\n",
        "          aux_ref_i[:, pl.dslice(pl.multiple_of(pair_offset, half_length), half_length)] = aux_outputs[i_aux][i][0]\n",
        "          aux_ref_i[:, pl.dslice(pl.multiple_of(pair_offset + half_length, half_length), half_length)] = aux_outputs[i_aux][i][1]\n",
        "\n",
        "\n",
        "def _compute_subtile_substages(\n",
        "    array_ref,\n",
        "    num_substages: int,\n",
        "    stage: int,\n",
        "    sort_order: int,\n",
        "    aux_refs=(),\n",
        "    dim1_offset: int = 0,\n",
        "):\n",
        "  \"\"\"Execute multiple substages of bitonic sort where compared values are from the same tile.\"\"\"\n",
        "  index = lax.broadcasted_iota(jnp.int32, (NUM_SUBLANES, NUM_LANES), 1)\n",
        "  tile_independent = num_substages < LOG_LANES\n",
        "\n",
        "  def compute_substage(substage, all_tiles):\n",
        "    tiles, *auxs_tiles = all_tiles\n",
        "    is_right_half = create_bit_indicator(substage)\n",
        "\n",
        "    if sort_order == 0:\n",
        "      base_should_swap = is_right_half\n",
        "    elif sort_order == 1:\n",
        "      base_should_swap = ~is_right_half\n",
        "    elif sort_order == 2:\n",
        "      if tile_independent:\n",
        "        base_should_swap = jnp.bitwise_xor(is_right_half, create_bit_indicator(stage))\n",
        "      else:\n",
        "        base_should_swap = None\n",
        "\n",
        "    permutation = jnp.bitwise_xor(index, 1 << substage)\n",
        "    permuted_tiles, *permuted_auxs_tiles = ([gather_2d(tile, permutation) for tile in t] for t in (tiles, *auxs_tiles))\n",
        "\n",
        "    output_tiles = []\n",
        "    output_auxs_tiles = [[] for _ in auxs_tiles]\n",
        "    flip_frequency = 1 << stage\n",
        "\n",
        "    for tile_idx, (tile, permuted_tile) in enumerate(zip(tiles, permuted_tiles, strict=True)):\n",
        "      if sort_order == 2 and not tile_independent:\n",
        "        tile_offset = dim1_offset + (tile_idx * NUM_LANES)\n",
        "        # only i32,i32->i1 lowers, not i1,i1->i1 so we upcast and then downcast\n",
        "        should_swap_local = jnp.where(\n",
        "            (tile_offset & flip_frequency) > 0,\n",
        "            (~is_right_half).astype(jnp.int32),\n",
        "            is_right_half.astype(jnp.int32)\n",
        "        ).astype(bool)\n",
        "      else:\n",
        "        should_swap_local = base_should_swap\n",
        "\n",
        "      condition = (tile > permuted_tile) == should_swap_local\n",
        "      output_tiles.append(\n",
        "          jnp.where(condition, tile, permuted_tile)\n",
        "      )\n",
        "      for aux_tiles, permuted_aux_tiles, output_aux_tiles in zip(auxs_tiles, permuted_auxs_tiles, output_auxs_tiles):\n",
        "        output_aux_tiles.append(\n",
        "          jnp.where(condition, aux_tiles[tile_idx], permuted_aux_tiles[tile_idx])\n",
        "        )\n",
        "    return (output_tiles, *output_auxs_tiles)\n",
        "\n",
        "  assert num_substages <= LOG_LANES\n",
        "  all_tiles = [split_array_to_tiles(ref) for ref in (array_ref, *aux_refs)]\n",
        "\n",
        "  for i in range(num_substages):\n",
        "    substage = num_substages - 1 - i\n",
        "    all_tiles = compute_substage(substage, all_tiles)\n",
        "\n",
        "  return (join_tiles_to_array(array_ref.shape, t) for t in all_tiles)\n",
        "\n",
        "def compute_subtile_substages(\n",
        "    array_ref,\n",
        "    *,\n",
        "    num_substages: int,\n",
        "    stage: int,\n",
        "    sort_order: int,\n",
        "    aux_refs=(),\n",
        "    unroll: int = 256,\n",
        "    dim1_offset: int = 0,\n",
        "    dim1_length: jax.Array = None,\n",
        "    slice_dim1: int = None\n",
        "):\n",
        "  \"\"\"Orchestrate subtile sorting operations with proper blocking.\"\"\"\n",
        "  if slice_dim1 is None:\n",
        "    slice_dim1 = min(unroll * NUM_LANES, array_ref.shape[1])\n",
        "  if dim1_length is None:\n",
        "    dim1_length = array_ref.shape[1]\n",
        "\n",
        "  unroll_dim0 = (unroll * NUM_LANES) // slice_dim1\n",
        "  slice_dim0 = min(unroll_dim0 * NUM_SUBLANES, array_ref.shape[0])\n",
        "  unroll = (slice_dim0 * slice_dim1) // (NUM_SUBLANES * NUM_LANES)\n",
        "\n",
        "  grid_dim0 = array_ref.shape[0] // slice_dim0\n",
        "  grid_dim1 = dim1_length // slice_dim1\n",
        "\n",
        "  @loop(0, grid_dim0 * grid_dim1)\n",
        "  def process_block(loop_idx):\n",
        "    block_row = loop_idx // grid_dim1\n",
        "    block_col = loop_idx % grid_dim1\n",
        "\n",
        "    array_ref_slice, *aux_ref_slices= (ref.at[\n",
        "        pl.dslice(block_row * slice_dim0, slice_dim0),\n",
        "        pl.dslice(block_col * slice_dim1, slice_dim1)\n",
        "    ] for ref in (array_ref, *aux_refs))\n",
        "\n",
        "    outputs = _compute_subtile_substages(\n",
        "        array_ref_slice,\n",
        "        aux_refs=aux_ref_slices,\n",
        "        num_substages=num_substages,\n",
        "        stage=stage,\n",
        "        sort_order=sort_order,\n",
        "        dim1_offset=dim1_offset + (block_col * slice_dim1)\n",
        "    )\n",
        "    for ref, output in zip((array_ref_slice, *aux_ref_slices), outputs):\n",
        "      ref[...] = output\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Bitonic Sort - Stage Orchestration\n",
        "# ============================================================================\n",
        "\n",
        "def compute_stages(\n",
        "    start_stage: int,\n",
        "    end_stage: int,\n",
        "    array_ref,\n",
        "    sort_order: int,\n",
        "    aux_refs=(),\n",
        "    unroll_crosstile: int = 64,\n",
        "    unroll_subtile: int = 64,\n",
        "    dim1_offset: int = 0\n",
        "):\n",
        "  \"\"\"Execute a range of bitonic sorting stages.\"\"\"\n",
        "  log_n = _log2(array_ref.shape[-1])\n",
        "\n",
        "  @loop(start_stage, end_stage)\n",
        "  def run_stage(stage):\n",
        "    for i in range(log_n):\n",
        "      substage = log_n - 1 - i\n",
        "      if (substage >= LOG_LANES):\n",
        "        @pl.when(stage > substage)\n",
        "        def _():\n",
        "          compute_crosstile_substage(\n",
        "              array_ref,\n",
        "              aux_refs=aux_refs,\n",
        "              substage=substage,\n",
        "              stage=stage,\n",
        "              sort_order=sort_order,\n",
        "              unroll=unroll_crosstile,\n",
        "              dim1_offset=dim1_offset\n",
        "          )\n",
        "      elif substage == (LOG_LANES - 1):\n",
        "        @pl.when(stage > substage)\n",
        "        def _():\n",
        "          compute_subtile_substages(\n",
        "              array_ref,\n",
        "              aux_refs=aux_refs,\n",
        "              num_substages=substage + 1,\n",
        "              stage=stage, sort_order=sort_order,\n",
        "              dim1_offset=dim1_offset,\n",
        "              unroll=unroll_subtile,\n",
        "          )\n",
        "      else:\n",
        "        @pl.when(stage  == (substage+1))\n",
        "        def _():\n",
        "          compute_subtile_substages(\n",
        "              array_ref,\n",
        "              aux_refs=aux_refs,\n",
        "              num_substages=substage + 1,\n",
        "              stage=stage, sort_order=sort_order,\n",
        "              dim1_offset=dim1_offset,\n",
        "              unroll=unroll_subtile,\n",
        "          )\n",
        "\n",
        "\n",
        "def bitonic_sort(\n",
        "    array_ref,\n",
        "    aux_refs = (),\n",
        "    k: int = None,\n",
        "    dim: int = None,\n",
        "    descending: bool = False\n",
        "):\n",
        "  \"\"\"Core bitonic sort implementation.\"\"\"\n",
        "  if dim is None:\n",
        "    dim = len(array_ref.shape) - 1\n",
        "  if dim != len(array_ref.shape) - 1:\n",
        "    raise ValueError(\"Only sorting along the last dimension is supported\")\n",
        "\n",
        "  if k is None:\n",
        "    k = array_ref.shape[-1]\n",
        "\n",
        "  log_n = _log2(array_ref.shape[dim])\n",
        "  if 2**log_n != array_ref.shape[dim]:\n",
        "    raise ValueError(\"Size along sort dimension must be a power of 2\")\n",
        "\n",
        "  log_k = _log2(k)\n",
        "  if 2**log_k != k:\n",
        "    raise ValueError(\"k must be a power of 2\")\n",
        "\n",
        "  if array_ref.shape[0] > 2**15:\n",
        "    raise ValueError(\"Index packing requires shape[0] <= 32768\")\n",
        "\n",
        "  # Execute bitonic stages\n",
        "  compute_stages(\n",
        "    1, log_n, array_ref,\n",
        "    aux_refs=aux_refs,\n",
        "    sort_order=2)\n",
        "  compute_stages(\n",
        "    log_n, log_n + 1, array_ref,\n",
        "    aux_refs=aux_refs,\n",
        "    sort_order=(1 if descending else 0)\n",
        "  )\n",
        "\n",
        "\n",
        "def sort_kernel(\n",
        "    input_ref,\n",
        "    output_ref,\n",
        "    output_index_ref,\n",
        "    *,\n",
        "    descending: bool\n",
        "):\n",
        "  \"\"\"Pallas kernel for sorting.\"\"\"\n",
        "  return_indices = output_index_ref is not None\n",
        "  pack_indices = (input_ref.dtype==jnp.bfloat16 and input_ref.shape[-1]<2**16)\n",
        "  use_index_scratch=return_indices and not pack_indices and not (output_index_ref.shape==input_ref.shape and output_index_ref.dtype==jnp.int32)\n",
        "  @functools.partial(pl.run_scoped,\n",
        "    float32_scratch_ref=pltpu.VMEM(input_ref.shape, jnp.float32) if input_ref.dtype != jnp.float32 else None,\n",
        "    index_scratch_ref=pltpu.VMEM(input_ref.shape, jnp.int32) if use_index_scratch else None,\n",
        "  )\n",
        "  def _(float32_scratch_ref, index_scratch_ref):\n",
        "    if float32_scratch_ref is None:\n",
        "      working_ref = input_ref\n",
        "    else:\n",
        "      working_ref = float32_scratch_ref\n",
        "      working_ref[...] = input_ref[...].astype(jnp.float32)\n",
        "\n",
        "    aux_refs = []\n",
        "    if return_indices:\n",
        "      indices = jax.lax.broadcasted_iota(jnp.int32, input_ref.shape, 1)\n",
        "      if pack_indices:\n",
        "        working_ref[...] = pack_value_with_index(working_ref[...], indices)\n",
        "        assert index_scratch_ref is None\n",
        "      else:\n",
        "        if index_scratch_ref is not None:\n",
        "          index_ref = index_scratch_ref\n",
        "        else:\n",
        "          index_ref = output_index_ref\n",
        "        index_ref[...] = indices\n",
        "        aux_refs.append(index_ref)\n",
        "\n",
        "    k = output_ref.shape[-1]\n",
        "    bitonic_sort(\n",
        "      working_ref,\n",
        "      aux_refs=aux_refs,\n",
        "      k=max(k, NUM_LANES),\n",
        "      descending=descending)\n",
        "\n",
        "    if return_indices:\n",
        "      if pack_indices:\n",
        "        values, indices = unpack_value_and_index(working_ref[...])\n",
        "      else:\n",
        "        indices = index_ref\n",
        "        values = working_ref\n",
        "      output_index_ref[...] = indices[..., :k].astype(output_index_ref.dtype)\n",
        "    else:\n",
        "      values = working_ref\n",
        "\n",
        "    output_ref[...] = values[..., :k].astype(output_ref.dtype)\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jit,\n",
        "    static_argnames=(\"k\", \"block_size\", \"output_dtype\", \"return_indices\", \"inplace\", \"descending\")\n",
        ")\n",
        "def sort_pallas(\n",
        "    x,\n",
        "    k=None,\n",
        "    block_size=None,\n",
        "    return_indices=False,\n",
        "    output_dtype=None,\n",
        "    inplace=False,\n",
        "    descending=False\n",
        "):\n",
        "  \"\"\"\n",
        "  High-level interface for Pallas-based sorting on TPU.\n",
        "\n",
        "  Args:\n",
        "      x: Input array to sort (2D)\n",
        "      k: Number of top elements to return (default: all)\n",
        "      block_size: Token blocking size for memory efficiency\n",
        "      return_indices: Whether to return original indices\n",
        "      output_dtype: Output data type\n",
        "      inplace: Whether to modify input array\n",
        "      descending: Sort in descending order\n",
        "  \"\"\"\n",
        "  if x.ndim != 2:\n",
        "    raise ValueError('Only 2D inputs supported')\n",
        "\n",
        "  if k is None:\n",
        "    k = x.shape[-1]\n",
        "\n",
        "  if inplace and k!=x.shape[-1]:\n",
        "    raise ValueError('Cannot reuse input buffer if topk requested')\n",
        "\n",
        "  if block_size is None:\n",
        "    block_size = min(max(NUM_SUBLANES, (2**14) // x.shape[-1]), x.shape[0])\n",
        "\n",
        "  if x.dtype not in (jnp.bfloat16, jnp.float32):\n",
        "    raise NotImplementedError('Only f32 and bf16 inputs supported')\n",
        "\n",
        "  if output_dtype is None:\n",
        "    output_dtype = x.dtype\n",
        "\n",
        "  output_shapes = (\n",
        "      jax.ShapeDtypeStruct((x.shape[0], k), output_dtype),\n",
        "      jax.ShapeDtypeStruct((x.shape[0], k), jnp.int32) if return_indices else None,\n",
        "  )\n",
        "\n",
        "  input_spec = pl.BlockSpec((block_size, x.shape[-1]), lambda i: (i, 0))\n",
        "  output_specs = (\n",
        "      pl.BlockSpec((block_size, k), lambda i: (i, 0)),\n",
        "      pl.BlockSpec((block_size, k), lambda i: (i, 0)) if return_indices else None,\n",
        "  )\n",
        "\n",
        "  val, index = pl.pallas_call(\n",
        "      functools.partial(sort_kernel, descending=descending),\n",
        "      out_shape=output_shapes,\n",
        "      in_specs=(input_spec,),\n",
        "      out_specs=output_specs,\n",
        "      grid=(x.shape[0] // block_size,),\n",
        "      input_output_aliases={0: 0} if inplace else {},\n",
        "      compiler_params=dict(\n",
        "        mosaic=dict(\n",
        "          vmem_limit_bytes=int(0.9 * 2**27)))\n",
        "  )(x)\n",
        "  if return_indices:\n",
        "    return val, index\n",
        "  return val\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Large Array Sorting (Multi-Stage HBM)\n",
        "# ============================================================================\n",
        "\n",
        "class AsyncCopyAggregator:\n",
        "  \"\"\"Bundles multiple async copy operations as a single copy operation.\"\"\"\n",
        "\n",
        "  def __init__(self, copy_descriptors):\n",
        "    self.copy_descriptors = tuple(copy_descriptors)\n",
        "\n",
        "  def wait(self):\n",
        "    \"\"\"Wait for all copy operations to complete.\"\"\"\n",
        "    for descriptor in self.copy_descriptors:\n",
        "      descriptor.wait()\n",
        "\n",
        "\n",
        "def _substage_hbm_kernel(\n",
        "    input_hbm_ref,\n",
        "    aux_input_hbm_refs,\n",
        "    substage_ref,\n",
        "    stage_ref,\n",
        "    output_hbm_ref,\n",
        "    aux_output_hbm_refs,\n",
        "    input_semaphores,\n",
        "    output_semaphores,\n",
        "    input_vmem_ref,\n",
        "    aux_input_vmem_refs,\n",
        "    output_vmem_ref,\n",
        "    aux_output_vmem_refs,\n",
        "    sort_order: int\n",
        "):\n",
        "  \"\"\"Kernel for running a substage which do not fit in VMEM.\"\"\"\n",
        "  # Handle sublane dimension indexing\n",
        "  sublane_block = input_vmem_ref.shape[-2]\n",
        "  sublane_slice = pl.dslice(pl.program_id(0) * sublane_block, sublane_block)\n",
        "  input_hbm_ref, aux_input_hbm_refs, output_hbm_ref, aux_output_hbm_refs = jax.tree.map(lambda ref: ref.at[sublane_slice], (\n",
        "      input_hbm_ref, aux_input_hbm_refs, output_hbm_ref, aux_output_hbm_refs,\n",
        "  ))\n",
        "\n",
        "\n",
        "  substage = substage_ref[0]\n",
        "  stage = stage_ref[0]\n",
        "  slice_length = input_vmem_ref.shape[-1]\n",
        "  array_length = input_hbm_ref.shape[-1]\n",
        "  pair_length = 2 ** (substage + 1)\n",
        "  num_pairs = array_length // pair_length\n",
        "  slices_per_pair = (pair_length // 2) // slice_length\n",
        "\n",
        "  def compute_start_index(i):\n",
        "    pair_idx = i // slices_per_pair\n",
        "    pair_subslice_idx = i % slices_per_pair\n",
        "    return pair_idx * pair_length + pair_subslice_idx * slice_length\n",
        "\n",
        "  def perform_dma(i, is_load):\n",
        "    \"\"\"Perform DMA operation (load or store).\"\"\"\n",
        "    buffer_slot = lax.rem(i, 2)\n",
        "    left_start = compute_start_index(i)\n",
        "    right_start = left_start + (pair_length // 2)\n",
        "    input_dma_refs = (\n",
        "      (input_hbm_ref, *aux_input_hbm_refs),\n",
        "      (input_vmem_ref, *aux_input_vmem_refs)\n",
        "    )\n",
        "    output_dma_refs = (\n",
        "      (output_hbm_ref, *aux_output_hbm_refs),\n",
        "      (output_vmem_ref, *aux_output_vmem_refs)\n",
        "    )\n",
        "    copies = []\n",
        "    for i_ref, (hbm_ref, vmem_ref) in enumerate(zip(\n",
        "        *(input_dma_refs if is_load else output_dma_refs),\n",
        "        strict=True\n",
        "      )):\n",
        "\n",
        "        for vmem_slot, start in enumerate((left_start, right_start)):\n",
        "          # Compiler fails to recognize start indices are multiples of num_lanes, so we tell the compiler explicitly\n",
        "          start = pl.multiple_of(start, NUM_LANES)\n",
        "          hbm_ref_slice = hbm_ref.at[:, pl.dslice(start, slice_length)]\n",
        "          vmem_ref_slice = vmem_ref.at[buffer_slot, vmem_slot]\n",
        "          sem = (input_semaphores if is_load else output_semaphores).at[buffer_slot, vmem_slot, i_ref]\n",
        "          src, dst = (hbm_ref_slice, vmem_ref_slice) if is_load else (vmem_ref_slice, hbm_ref_slice)\n",
        "          copies.append(\n",
        "            pltpu.async_copy(\n",
        "              src_ref=src,\n",
        "              dst_ref=dst,\n",
        "              sem=sem,\n",
        "          ))\n",
        "    return AsyncCopyAggregator(copies)\n",
        "\n",
        "  load_dma = functools.partial(perform_dma, is_load=True)\n",
        "  store_dma = functools.partial(perform_dma, is_load=False)\n",
        "\n",
        "  def compute_comparison(loop_idx):\n",
        "    \"\"\"Perform comparison and swap logic.\"\"\"\n",
        "    start_idx = compute_start_index(loop_idx)\n",
        "    slot = lax.rem(loop_idx, 2)\n",
        "    left, right = input_vmem_ref[slot]\n",
        "\n",
        "    if sort_order != 2:\n",
        "      is_descending = (sort_order == 1)\n",
        "      mask = (left > right) if is_descending else (left < right)\n",
        "    else:\n",
        "      is_descending = create_bit_indicator(stage, start_idx)\n",
        "      mask = jnp.bitwise_xor(is_descending, left < right)\n",
        "\n",
        "    for i, m in enumerate((mask, ~mask)):\n",
        "      output_vmem_ref[slot, i] = jnp.where(m, left, right)\n",
        "\n",
        "      for (aux_input_ref, aux_output_ref) in zip(aux_input_vmem_refs, aux_output_vmem_refs):\n",
        "        aux_output_ref[slot, i] = jnp.where(m, *aux_input_ref[slot])\n",
        "\n",
        "  num_iterations = input_hbm_ref.shape[-1] // (2 * slice_length)\n",
        "  assert num_iterations > 0\n",
        "\n",
        "  # Pipeline: Load -> Compute -> Store\n",
        "  initial_load = load_dma(0)\n",
        "  if num_iterations > 1:\n",
        "    next_load = load_dma(1)\n",
        "\n",
        "  initial_load.wait()\n",
        "  compute_comparison(0)\n",
        "\n",
        "  if num_iterations == 1:\n",
        "    store_dma(0).wait()\n",
        "    return\n",
        "\n",
        "  next_load.wait()\n",
        "\n",
        "  @loop(1, num_iterations - 1)\n",
        "  def pipeline_iteration(loop_idx):\n",
        "    store_op = store_dma(loop_idx - 1)\n",
        "    load_op = load_dma(loop_idx + 1)\n",
        "    compute_comparison(loop_idx)\n",
        "    store_op.wait()\n",
        "    load_op.wait()\n",
        "\n",
        "  store_op = store_dma(num_iterations - 2)\n",
        "  compute_comparison(num_iterations - 1)\n",
        "  store_op.wait()\n",
        "  store_dma(num_iterations - 1).wait()\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.jit,\n",
        "    static_argnames=('sort_order', 'block_shape', 'inplace')\n",
        ")\n",
        "def compute_substage_vmem_efficient(\n",
        "    x,\n",
        "    substage: int,\n",
        "    stage: int,\n",
        "    sort_order: int,\n",
        "    auxs=(),\n",
        "    block_shape=None,\n",
        "    inplace=True,\n",
        "    runtime_checks=False,\n",
        "):\n",
        "  \"\"\"Runs a substage without loading the full lane dimension into VMEM.\"\"\"\n",
        "  if block_shape is None:\n",
        "    block_shape = (NUM_SUBLANES, 2**16)\n",
        "  if runtime_checks:\n",
        "    checkify.check(substage >= LOG_LANES, 'Intra tile comparisons not supported')\n",
        "    slice_length = block_shape[-1]\n",
        "    checkify.check(slice_length <= 2**substage, 'invalid slice length, sections of length {} (2**substage) sliced into chunks of size 'f'{slice_length}', 2**substage)\n",
        "    checkify.check(substage < stage, 'substage greater than stage is not valid, substage={}, stage={}', substage, stage)\n",
        "\n",
        "  # HBM-VMEM transfers handled manually as loading and storing two blocks from the same array (inplace) is not expressible in BlockSpecs\n",
        "  input_specs = (\n",
        "      pl.BlockSpec(memory_space=pltpu.ANY),\n",
        "      (pl.BlockSpec(memory_space=pltpu.ANY),)*len(auxs),\n",
        "      pl.BlockSpec(memory_space=pltpu.SMEM),\n",
        "      pl.BlockSpec(memory_space=pltpu.SMEM),\n",
        "  )\n",
        "\n",
        "  output_shape = (\n",
        "    jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "    jax.tree.map(lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype), auxs)\n",
        "  )\n",
        "  num_refs = 1+len(auxs)\n",
        "  aux_vmems = jax.tree.map(lambda x: pltpu.VMEM((2, 2, *block_shape), x.dtype), auxs)\n",
        "\n",
        "  return pl.pallas_call(\n",
        "      functools.partial(\n",
        "          _substage_hbm_kernel,\n",
        "          sort_order=sort_order\n",
        "      ),\n",
        "      # indexing in outer loop over sublane dimension is handled inside the kernel, as pltpu.ANY memory space doesnt support block specs\n",
        "      grid=(x.shape[0] // block_shape[0],),\n",
        "      out_shape=output_shape,\n",
        "      in_specs=input_specs,\n",
        "      out_specs=input_specs[:2],\n",
        "      input_output_aliases={0: 0} if inplace else {},\n",
        "      # (2,2) = (slot, left/right array for swap)\n",
        "      scratch_shapes=(\n",
        "          pltpu.SemaphoreType.DMA((2, 2, num_refs)),\n",
        "          pltpu.SemaphoreType.DMA((2, 2, num_refs)),\n",
        "          pltpu.VMEM((2, 2, *block_shape), jnp.float32),\n",
        "          aux_vmems,\n",
        "          pltpu.VMEM((2, 2, *block_shape), jnp.float32),\n",
        "          aux_vmems,\n",
        "      ),\n",
        "      compiler_params=dict(\n",
        "          mosaic=dict(\n",
        "              vmem_limit_bytes=int(0.9 * 2**29)\n",
        "          )\n",
        "      )\n",
        "  )(x, auxs, substage[None], stage[None])\n",
        "\n",
        "\n",
        "def subsort_kernel(\n",
        "    input_ref,\n",
        "    aux_refs,\n",
        "    stage_ref,\n",
        "    output_ref,\n",
        "    aux_output_refs,\n",
        "    float32_scratch_ref,\n",
        "    *,\n",
        "    sort_order: int\n",
        "):\n",
        "  \"\"\"Kernel for sorting subsequences of input for substages which fit in VMEM.\"\"\"\n",
        "  if float32_scratch_ref is None:\n",
        "    assert input_ref.dtype == jnp.float32\n",
        "    working_ref = input_ref\n",
        "  else:\n",
        "    working_ref = float32_scratch_ref\n",
        "    working_ref[...] = input_ref[...].astype(jnp.float32)\n",
        "\n",
        "  # to keep track of global index for bitonic sort order (based off stage)\n",
        "  dim1_offset = pl.program_id(1) * input_ref.shape[-1]\n",
        "\n",
        "  if stage_ref is None:\n",
        "    # Run all stages from 1 to log2(length)\n",
        "    # All stages run with songle sort order (a 'normal' sort is bitonic order until the last stage which is ascending/descending)\n",
        "    compute_stages(\n",
        "      1, _log2(input_ref.shape[-1]),\n",
        "      working_ref,\n",
        "      aux_refs=aux_refs,\n",
        "      sort_order=sort_order,\n",
        "      dim1_offset=dim1_offset\n",
        "    )\n",
        "  else:\n",
        "    # Run a single stage\n",
        "    stage = stage_ref[0]\n",
        "    compute_stages(\n",
        "        stage, stage + 1,\n",
        "        working_ref,\n",
        "        aux_refs=aux_refs,\n",
        "        sort_order=sort_order,\n",
        "        dim1_offset=dim1_offset\n",
        "    )\n",
        "  output_ref[...] = working_ref[...].astype(output_ref.dtype)\n",
        "  for in_ref, out_ref in zip(aux_refs, aux_output_refs):\n",
        "    out_ref[...] = in_ref[...]\n",
        "\n",
        "@functools.partial(\n",
        "    jit,\n",
        "    static_argnames=(\"block_token\", \"num_substages\", \"sort_order\")\n",
        ")\n",
        "def compute_substages(\n",
        "    x,\n",
        "    stage,\n",
        "    num_substages: int,\n",
        "    sort_order: int,\n",
        "    auxs=(),\n",
        "    block_token=None\n",
        "):\n",
        "  \"\"\"\n",
        "  Runs substages from num_substages-1 down to 0 as part of a stage\n",
        "\n",
        "  Args:\n",
        "      x: Input array\n",
        "      stage: Specific stage to run (or None to run stages 1 to 'num_substages')\n",
        "      num_substages: how many substages to run\n",
        "      sort_order: 0=ascending, 1=descending, 2=bitonic\n",
        "      block_size: Token blocking size\n",
        "  \"\"\"\n",
        "  if x.ndim != 2:\n",
        "    raise ValueError('Only 2D inputs supported')\n",
        "\n",
        "  if block_token is None:\n",
        "    block_token = NUM_SUBLANES\n",
        "\n",
        "  subsequence_length = 2**num_substages\n",
        "  if subsequence_length > 2**19:\n",
        "    raise ValueError('block size exceeds VMEM limits, max subsequence length is 524288')\n",
        "  x_block_spec = pl.BlockSpec((block_token, subsequence_length), lambda i, j: (i, j))\n",
        "  in_specs=(\n",
        "      x_block_spec,\n",
        "      jax.tree.map(lambda _: x_block_spec, auxs),\n",
        "      pl.BlockSpec(memory_space=pltpu.SMEM) if stage is not None else None,\n",
        "  )\n",
        "  return pl.pallas_call(\n",
        "      functools.partial(subsort_kernel, sort_order=sort_order),\n",
        "      out_shape=(\n",
        "        jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "        jax.tree.map(lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype), auxs),\n",
        "      ),\n",
        "      in_specs=in_specs,\n",
        "      out_specs=in_specs[:2],\n",
        "      scratch_shapes=(\n",
        "          pltpu.VMEM((block_token, subsequence_length), jnp.float32)\n",
        "          if x.dtype != jnp.float32 else None,\n",
        "      ),\n",
        "      grid=(x.shape[0] // block_token, x.shape[-1] // subsequence_length),\n",
        "      compiler_params=dict(\n",
        "        mosaic=dict(\n",
        "          vmem_limit_bytes=int(0.9 * 2**27)))\n",
        "  )(x, auxs, stage[None] if stage is not None else None)\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnames=('num_vmem_substages', 'descending', 'return_indices'))\n",
        "def sort_pallas_vmem_efficient(x, num_vmem_substages=19, descending=False, return_indices=False):\n",
        "  \"\"\"\n",
        "  Sort large arrays using a hybrid HBM-VMEM approach.\n",
        "\n",
        "  This function handles arrays larger than VMEM by breaking them into\n",
        "  subsections, sorting in VMEM, then merging with HBM-based operations.\n",
        "\n",
        "  Args:\n",
        "      x: Input array to sort\n",
        "      num_vmem_substages: log2 of max size that fits in VMEM (default: 2^19)\n",
        "      descending: Sort in descending order\n",
        "  \"\"\"\n",
        "  num_stages = _log2(x.shape[-1])\n",
        "\n",
        "  # If array fits in VMEM, use simple sort\n",
        "  if num_stages <= num_vmem_substages:\n",
        "    return sort_pallas(x, descending=descending, return_indices=return_indices)\n",
        "\n",
        "  auxs = (jax.lax.broadcasted_iota(jnp.int32, x.shape, 1),) if return_indices else ()\n",
        "\n",
        "  def run_stage(stage, carry, *, sort_order):\n",
        "    \"\"\"Execute a complete sorting stage.\"\"\"\n",
        "    def _compute_substage_vmem_efficient_body(i, carry):\n",
        "      substage = stage - 1 - i\n",
        "      x, auxs = carry\n",
        "      return compute_substage_vmem_efficient(\n",
        "          x, substage, stage, auxs=auxs, sort_order=sort_order\n",
        "      )\n",
        "\n",
        "    # First: HBM-based substages for cross-VMEM-block operations\n",
        "    x, auxs = jax.lax.fori_loop(0, stage - num_vmem_substages, _compute_substage_vmem_efficient_body, carry)\n",
        "\n",
        "    # Then: VMEM-based substages for within-block operations\n",
        "    return compute_substages(x, auxs=auxs, stage=stage, num_substages=num_vmem_substages, sort_order=sort_order)\n",
        "\n",
        "  # Initial bitonic sorting of VMEM-sized blocks up to VMEM-sized subsequences\n",
        "  carry = compute_substages(x, auxs=auxs, stage=None, num_substages=num_vmem_substages, sort_order=2)\n",
        "\n",
        "  # Merge blocks through successive stages\n",
        "  carry = jax.lax.fori_loop(\n",
        "      num_vmem_substages, num_stages,\n",
        "      functools.partial(run_stage, sort_order=2), carry\n",
        "  )\n",
        "\n",
        "  # Final stage determines overall sort direction\n",
        "  x, auxs = run_stage(num_stages, carry, sort_order=int(descending))\n",
        "  if return_indices:\n",
        "    return x, auxs[0]\n",
        "  return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Top-K Operations\n",
        "# ============================================================================\n",
        "\n",
        "def blockwise_topk(\n",
        "    logits,\n",
        "    k: int,\n",
        "    block_topk_values=None,\n",
        "    block_topk_indices=None,\n",
        "    start_k: int = 0,\n",
        "    num_blocks: int = NUM_LANES,\n",
        "    mode: str = \"jax\",\n",
        "):\n",
        "  \"\"\"\n",
        "  Compute blockwise top-k using a sinking sort approach.\n",
        "\n",
        "  Args:\n",
        "      logits: Input logits to find top-k from\n",
        "      k: Number of top elements to find\n",
        "      block_topk_values: Pre-allocated buffers for values\n",
        "      block_topk_indices: Pre-allocated buffers for indices\n",
        "      start_k: Starting position (for incremental top-k)\n",
        "      num_blocks: Number of blocks to process\n",
        "      mode: \"jax\" or \"pallas\" execution mode\n",
        "  \"\"\"\n",
        "  num_tokens = logits.shape[0]\n",
        "\n",
        "  if start_k != 0 and (block_topk_values is None or block_topk_indices is None):\n",
        "    raise ValueError(\n",
        "        \"start_k > 0 requires pre-computed buffers in \"\n",
        "        \"block_topk_values and block_topk_indices\"\n",
        "    )\n",
        "\n",
        "  if mode == \"jax\":\n",
        "    block_topk_values = [\n",
        "        jnp.full(\n",
        "            (num_tokens, num_blocks),\n",
        "            jnp.finfo(logits.dtype).min,\n",
        "            dtype=logits.dtype\n",
        "        )\n",
        "        for _ in range(k)\n",
        "    ]\n",
        "    block_topk_indices = [\n",
        "        jnp.full((num_tokens, num_blocks), 0, dtype=jnp.int32)\n",
        "        for _ in range(k)\n",
        "    ]\n",
        "  elif mode == \"pallas\":\n",
        "    if block_topk_values is None or block_topk_indices is None:\n",
        "      raise ValueError(\n",
        "          \"Pallas mode requires pre-allocated buffers\"\n",
        "      )\n",
        "\n",
        "  def process_block(block_idx, carry):\n",
        "    \"\"\"Process a single tile with sinking sort.\"\"\"\n",
        "    values_list, indices_list = carry\n",
        "\n",
        "    # Extract current block\n",
        "    if mode == \"pallas\":\n",
        "      current_values = logits[..., pl.dslice(num_blocks * block_idx, num_blocks)]\n",
        "    elif mode == \"jax\":\n",
        "      current_values = jax.lax.dynamic_slice_in_dim(\n",
        "          logits, block_idx * num_blocks, num_blocks, axis=1\n",
        "      )\n",
        "    else:\n",
        "      raise ValueError(\"mode must be 'pallas' or 'jax'\")\n",
        "\n",
        "    current_indices = jnp.full((num_tokens, num_blocks), block_idx, jnp.int32)\n",
        "\n",
        "    # Sinking sort: compare and swap through k levels\n",
        "    for level in range(k):\n",
        "      if level < start_k:\n",
        "        # Invalidate already-found elements\n",
        "        current_values = jnp.where(\n",
        "            current_indices == indices_list[level],\n",
        "            float(\"-inf\"),\n",
        "            current_values\n",
        "        )\n",
        "      else:\n",
        "        # Exchange with stored top-k\n",
        "        mask = current_values > values_list[level]\n",
        "\n",
        "        values_list[level], current_values = (\n",
        "            jnp.where(m, current_values, values_list[level])\n",
        "            for m in (mask, ~mask)\n",
        "        )\n",
        "        indices_list[level], current_indices = (\n",
        "            jnp.where(m, current_indices, indices_list[level])\n",
        "            for m in (mask, ~mask)\n",
        "        )\n",
        "\n",
        "    return (values_list, indices_list)\n",
        "\n",
        "  return unrolled_fori_loop(\n",
        "      logits.shape[-1] // num_blocks,\n",
        "      process_block,\n",
        "      (block_topk_values, block_topk_indices),\n",
        "      unroll=16,\n",
        "  )\n",
        "\n",
        "\n",
        "def dense_gather_kernel(values_ref, indices_ref, output_ref):\n",
        "  \"\"\"Gather values by indexing in to all of value with a mask, rather than a single gather per index.\"\"\"\n",
        "  # TODO: consider fori_loop and unroll for large shapes\n",
        "  for token_offset in range(0, values_ref.shape[0], NUM_SUBLANES):\n",
        "    token_slice = pl.dslice(token_offset, NUM_SUBLANES)\n",
        "    output = jnp.zeros((NUM_SUBLANES, NUM_LANES), values_ref.dtype)\n",
        "    indices = indices_ref[token_offset: token_offset + NUM_SUBLANES]\n",
        "\n",
        "    for block_offset in range(0, values_ref.shape[1], NUM_LANES):\n",
        "      mask = (indices >= block_offset) & (indices < block_offset + NUM_LANES)\n",
        "      output = jnp.where(\n",
        "          mask,\n",
        "          gather_2d(\n",
        "              values_ref[\n",
        "                  token_offset: token_offset + NUM_SUBLANES,\n",
        "                  block_offset: block_offset + NUM_LANES\n",
        "              ],\n",
        "              indices % NUM_LANES\n",
        "          ),\n",
        "          output,\n",
        "      )\n",
        "\n",
        "    output_ref[token_slice] = output[:, :output_ref.shape[1]].astype(output_ref.dtype)\n",
        "\n",
        "\n",
        "def topk_from_packed(x_ref, k: int):\n",
        "  \"\"\"\n",
        "  Extract top-k from packed float32 array.\n",
        "\n",
        "  Args:\n",
        "      x_ref: Reference to packed array (bfloat16 values + uint16 indices)\n",
        "      k: Number of top elements to extract\n",
        "\n",
        "  Returns:\n",
        "      Tuple of (values, indices) for top-k elements\n",
        "  \"\"\"\n",
        "  assert x_ref.dtype == jnp.float32\n",
        "\n",
        "  iota = jax.lax.broadcasted_iota(jnp.int32, x_ref.shape, 1)\n",
        "  assert x_ref.shape[-1] < 2**16, \\\n",
        "      'Packing requires vocab size < 65536 for uint16 indices'\n",
        "  x_ref[...] = pack_value_with_index(x_ref[...], iota)\n",
        "  bitonic_sort(x_ref, k=max(k, NUM_LANES), descending=True)\n",
        "  values, indices = unpack_value_and_index(x_ref[...])\n",
        "  return values[:, :k], indices[:, :k]\n",
        "\n",
        "\n",
        "def topk_blockwise_superset_kernel(\n",
        "    logits_ref,\n",
        "    topk_values_ref,\n",
        "    topk_indices_ref,\n",
        "    max_depth_ref,\n",
        "    block_topm_values_ref,\n",
        "    block_topm_indices_ref,\n",
        "    termination_flag_ref,\n",
        "    k: int = 64,\n",
        "    block_topk_schedule: tuple[int] | None = None,\n",
        "    topk_schedule: tuple[int] | None = None,\n",
        "):\n",
        "  \"\"\"\n",
        "  Compute blockwise top-k supersets until global top-k is guaranteed.\n",
        "\n",
        "  This uses an adaptive algorithm that incrementally increases m until\n",
        "  the blockwise top-m's provably contain the global top-k.\n",
        "  \"\"\"\n",
        "  # Initialize buffers\n",
        "  block_size = logits_ref.shape[0]\n",
        "  shape = (block_size, block_topm_values_ref.shape[1])\n",
        "\n",
        "  token_slice = pl.dslice(pl.program_id(0) * block_size, block_size)\n",
        "\n",
        "  block_topm_values_ref[token_slice] = jnp.full(\n",
        "      shape, jnp.finfo(jnp.float32).min, dtype=jnp.float32\n",
        "  )\n",
        "  block_topm_indices_ref[token_slice] = jnp.full(shape, 0, dtype=jnp.int32)\n",
        "\n",
        "  for i in range(block_size):\n",
        "    max_depth_ref[pl.program_id(0) * block_size + i] = k\n",
        "\n",
        "  termination_flag_ref[0] = 0\n",
        "\n",
        "  # Schedule of progressively larger m values\n",
        "  if block_topk_schedule is None:\n",
        "    block_topk_schedule = (5, 7, 9, 12)\n",
        "  block_topk_schedule = (0,) + block_topk_schedule + (k,)\n",
        "\n",
        "  # Incremental blockwise top-k computation\n",
        "  for completed_m, target_m in zip(block_topk_schedule, block_topk_schedule[1:]):\n",
        "\n",
        "    @pl.when(termination_flag_ref[0] == 0)\n",
        "    def _():\n",
        "      # Compute blockwise top-m\n",
        "      topk_vals, topk_idxs = blockwise_topk(\n",
        "          logits_ref,\n",
        "          block_topk_values=[\n",
        "              block_topm_values_ref[\n",
        "                  token_slice, pl.dslice(i * NUM_LANES, NUM_LANES)\n",
        "              ].astype(jnp.float32)\n",
        "              for i in range(target_m)\n",
        "          ],\n",
        "          block_topk_indices=[\n",
        "              block_topm_indices_ref[\n",
        "                  token_slice, pl.dslice(i * NUM_LANES, NUM_LANES)\n",
        "              ]\n",
        "              for i in range(target_m)\n",
        "          ],\n",
        "          k=target_m,\n",
        "          num_blocks=NUM_LANES,\n",
        "          start_k=completed_m,\n",
        "          mode=\"pallas\",\n",
        "      )\n",
        "\n",
        "      # Store results\n",
        "      for i in range(completed_m, target_m):\n",
        "        block_topm_values_ref[\n",
        "            token_slice, pl.dslice(i * NUM_LANES, NUM_LANES)\n",
        "        ] = topk_vals[i].astype(block_topm_values_ref.dtype)\n",
        "        block_topm_indices_ref[\n",
        "            token_slice, pl.dslice(i * NUM_LANES, NUM_LANES)\n",
        "        ] = topk_idxs[i].astype(block_topm_indices_ref.dtype)\n",
        "\n",
        "      # Termination criterion:\n",
        "      # If top-(m-1) blocks contain >= k values larger than\n",
        "      # the m-th largest value, then top-k is guaranteed to be in top-(m-1)\n",
        "      pivot = topk_vals[target_m - 1].max(-1, keepdims=True)\n",
        "      num_larger = (\n",
        "          sum([(v >= pivot) for v in topk_vals[:target_m - 1]])\n",
        "          .astype(jnp.float32)\n",
        "          .sum(-1)\n",
        "      )\n",
        "\n",
        "      termination_flag_ref[0] = 0\n",
        "      for i in range(block_size):\n",
        "        contains_topk = num_larger[i] >= k\n",
        "        termination_flag_ref[0] += contains_topk\n",
        "\n",
        "        # Record depth when criterion was met\n",
        "        token_idx = pl.program_id(0) * block_size + i\n",
        "        current_max = max_depth_ref[token_idx]\n",
        "        max_depth_ref[token_idx] = jnp.where(\n",
        "            contains_topk & (current_max == k),\n",
        "            target_m - 1,\n",
        "            current_max\n",
        "        )\n",
        "\n",
        "      # Check if all tokens converged\n",
        "      @pl.when(termination_flag_ref[0] != block_size)\n",
        "      def _():\n",
        "        termination_flag_ref[0] = 0\n",
        "\n",
        "  # Final top-k extraction (done by last program)\n",
        "  @pl.when(pl.program_id(0) == (pl.num_programs(0) - 1))\n",
        "  def _():\n",
        "    # Find maximum depth across all tokens\n",
        "    max_depth = jnp.array(0)\n",
        "    for i in range(max_depth_ref.shape[0]):\n",
        "      max_depth = jnp.maximum(max_depth, max_depth_ref[i])\n",
        "\n",
        "    # Use appropriate sorting depth based on max_depth\n",
        "    for depth_lower, depth_upper in zip(topk_schedule, topk_schedule[1:]):\n",
        "\n",
        "      @pl.when((max_depth > depth_lower) & (max_depth <= depth_upper))\n",
        "      def _():\n",
        "        # Sort the blockwise superset\n",
        "        values, block_local_indices = topk_from_packed(\n",
        "            block_topm_values_ref.at[:, :depth_upper * NUM_LANES],\n",
        "            k=NUM_LANES\n",
        "        )\n",
        "        topk_values_ref[...] = values.astype(topk_values_ref.dtype)\n",
        "\n",
        "        # Reconstruct global indices\n",
        "        global_indices = (\n",
        "            block_topm_indices_ref[:, :depth_upper * NUM_LANES] * NUM_LANES\n",
        "        ) + (\n",
        "            jax.lax.broadcasted_iota(\n",
        "                jnp.int32,\n",
        "                block_topm_indices_ref[:, :depth_upper * NUM_LANES].shape,\n",
        "                1\n",
        "            ) % NUM_LANES\n",
        "        )\n",
        "\n",
        "        dense_gather_kernel(\n",
        "            global_indices, block_local_indices, topk_indices_ref\n",
        "        )\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jit,\n",
        "    static_argnames=(\"k\", \"block_size\", \"block_topk_schedule\", \"topk_schedule\"),\n",
        ")\n",
        "def topk_pallas(\n",
        "    logits,\n",
        "    k: int,\n",
        "    block_size: int = 8,\n",
        "    block_topk_schedule=None,\n",
        "    topk_schedule=None,\n",
        "):\n",
        "  \"\"\"\n",
        "  High-level interface for adaptive blockwise top-k on TPU.\n",
        "\n",
        "  Args:\n",
        "      logits: Input logits [num_tokens, vocab_size]\n",
        "      k: Number of top elements to find\n",
        "      block_size: Token blocking size\n",
        "      block_topk_schedule: Schedule of m values for blockwise top-m\n",
        "      topk_schedule: Schedule for final sorting depth\n",
        "\n",
        "  Returns:\n",
        "      Tuple of (values, indices) for top-k elements\n",
        "  \"\"\"\n",
        "  num_tokens, vocab_size = logits.shape\n",
        "\n",
        "  if num_tokens % block_size != 0:\n",
        "    raise ValueError(\"num_tokens must be divisible by block_size\")\n",
        "\n",
        "  if topk_schedule is None:\n",
        "    topk_schedule = (0, 8, k)\n",
        "\n",
        "  if k > NUM_LANES:\n",
        "    raise ValueError(f\"k cannot exceed {NUM_LANES}\")\n",
        "\n",
        "  output_shapes = (\n",
        "      jax.ShapeDtypeStruct((num_tokens, NUM_LANES), logits.dtype),\n",
        "      jax.ShapeDtypeStruct((num_tokens, NUM_LANES), jnp.int32),\n",
        "      jax.ShapeDtypeStruct((num_tokens,), jnp.int32),\n",
        "  )\n",
        "\n",
        "  output_specs = (\n",
        "      pl.BlockSpec(),\n",
        "      pl.BlockSpec(),\n",
        "      pl.BlockSpec(memory_space=pltpu.SMEM),\n",
        "  )\n",
        "\n",
        "  topk_vals, topk_idxs, depths = pl.pallas_call(\n",
        "      functools.partial(\n",
        "          topk_blockwise_superset_kernel,\n",
        "          k=k,\n",
        "          block_topk_schedule=block_topk_schedule,\n",
        "          topk_schedule=topk_schedule,\n",
        "      ),\n",
        "      in_specs=(\n",
        "          pl.BlockSpec((block_size, vocab_size), lambda i: (i, 0)),\n",
        "      ),\n",
        "      out_shape=output_shapes,\n",
        "      scratch_shapes=(\n",
        "          pltpu.VMEM((num_tokens, k * NUM_LANES), jnp.float32),\n",
        "          pltpu.VMEM((num_tokens, k * NUM_LANES), jnp.int32),\n",
        "          pltpu.SMEM((1,), jnp.int32),\n",
        "      ),\n",
        "      grid=(num_tokens // block_size,),\n",
        "      out_specs=output_specs,\n",
        "      compiler_params=dict(\n",
        "          mosaic=dict(\n",
        "              vmem_limit_bytes=int(0.9 * 2**27)\n",
        "          )\n",
        "      )\n",
        "  )(logits)\n",
        "\n",
        "  return topk_vals[:, :k], topk_idxs[:, :k]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "#\n",
        "\n",
        "\n",
        "import gzip\n",
        "from glob import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "k = 64\n",
        "num_queries = 32\n",
        "vocab_size = 201088\n",
        "hidden_dim = 2880\n",
        "\n",
        "logit_key, key_act, key_weight = jax.random.split(jax.random.key(0), 3)\n",
        "x = jax.random.normal(key_act, (num_queries, hidden_dim), dtype=jnp.bfloat16)\n",
        "w = jax.random.normal(key_weight, (hidden_dim, vocab_size), dtype=jnp.bfloat16)\n",
        "logits = jax.random.normal(\n",
        "    key_weight, (num_queries, vocab_size), dtype=jnp.float32\n",
        ").astype(jnp.bfloat16)\n",
        "\n",
        "topk_xla = jax.jit(jax.lax.top_k, static_argnames=(\"k\",))\n",
        "approx_topk_xla = jax.jit(jax.lax.approx_max_k, static_argnames=(\"k\",))\n",
        "sort_xla = jax.jit(jnp.sort)\n",
        "argsort_xla = jax.jit(jnp.argsort)\n",
        "@jax.jit\n",
        "def add_one(x):\n",
        "  return x+1\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "@functools.partial(jax.vmap, in_axes=(0, None))\n",
        "def matmul_and_topk_xla(x, w, k=k):\n",
        "  logits = x @ w\n",
        "  return jax.lax.top_k(logits, k)\n",
        "\n",
        "def benchmark(_run):\n",
        "  def run():\n",
        "    return jax.block_until_ready(_run())\n",
        "  run()\n",
        "  with jax.profiler.trace(\"/content/\"):\n",
        "    run()\n",
        "\n",
        "  path = sorted(glob(\"/content/plugins/profile/*/**.json.gz\"), key=os.path.getmtime)[-1]\n",
        "  trace = json.load(gzip.open(path))\n",
        "  df = pd.DataFrame(trace[\"traceEvents\"])\n",
        "  df = df[~df.name.isna()]\n",
        "  print(df[df.name.str.contains(\"jit_\")][['name', 'dur']])\n",
        "import itertools\n",
        "for dtype, n in itertools.product((\n",
        "    jnp.float32,\n",
        "    jnp.bfloat16,\n",
        "    ), (\n",
        "    2**13, 2**15,\n",
        "    2**17,\n",
        "    2**20,)):\n",
        "  y = jax.random.normal(jax.random.key(0), (2**5, n), dtype)\n",
        "  print('y shape, dtype ', y.shape, y.dtype )\n",
        "  def _run():\n",
        "    return (\n",
        "      sort_xla(y),\n",
        "      sort_pallas_vmem_efficient(y),\n",
        "      argsort_xla(y),\n",
        "      sort_pallas_vmem_efficient(y, return_indices=True),\n",
        "      add_one(y),\n",
        "    )\n",
        "\n",
        "  benchmark(_run)\n",
        "  check_sort = True\n",
        "  if check_sort:\n",
        "    a, b,c, d, *_ = _run()\n",
        "    print('sort ', y.shape, y.dtype)\n",
        "    #print(\"xla\", a)\n",
        "    #print(\"pallas\", b)\n",
        "    print('match: ', (a==b).mean())\n",
        "    print('argsort ', y.shape, y.dtype)\n",
        "    #print(\"xla\", c)\n",
        "    #print(\"pallas\", d[1])\n",
        "    print('match: ', (c==d[1]).mean())\n",
        "\n",
        "check = True\n",
        "\n",
        "def _run():\n",
        "  return (\n",
        "    add_one(logits),\n",
        "    topk_xla(logits, k=k),\n",
        "    topk_pallas(logits, k=k, block_size=8),\n",
        "    topk_pallas(logits, k=k, block_size=16),\n",
        "    # Not exact. Runtime varies with recall, here run with default 0.95\n",
        "    approx_topk_xla(logits, k=k),\n",
        "  )\n",
        "\n",
        "if check:\n",
        "  benchmark(_run)\n",
        "  print('topk', logits.shape, logits.dtype, k)\n",
        "  print(\"XLA: \", topk_xla(logits, k=k))\n",
        "  print(\"\\nPallas:\", topk_pallas(logits, k=k))\n",
        "  print(\n",
        "  [\n",
        "  (topk_xla(logits, k=k)[i] == topk_pallas(logits, k=k)[i]).mean() for i in range(2)\n",
        "  ]\n",
        "  )\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "  # Print performance statistics for various configurations\n",
        "  print(\"=== Performance Statistics ===\\n\")\n",
        "  calculate_performance_stats(16, 32, 468)\n",
        "  calculate_performance_stats(10, 1024, 300)\n",
        "  calculate_performance_stats(17, 32, 1019)\n",
        "  calculate_performance_stats(17, 32, 816)  # without supertile swaps\n",
        "  calculate_performance_stats(17, 256, 6444)  # without supertile swaps\n",
        "  calculate_performance_stats(13, 32, 45.5)\n",
        "\n",
        "  print(\"\\n=== Correctness Verification ===\\n\")\n",
        "  verify_correctness()\n",
        "\n",
        "  print(\"\\n=== Performance Profiling ===\\n\")\n",
        "  profile_and_analyze()\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}